{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training information retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, explode, col, countDistinct, collect_list, monotonically_increasing_id, row_number\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import os\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2\n",
    "\n",
    "from pyspark.sql.types import StructField, StructType, IntegerType, FloatType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right paths on local machine\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.7'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '8g') \\\n",
    "        .config('spark.executor.memory', '8g') \\\n",
    "        .config('spark.memory.fraction', '0.8') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.local.dir', '/home/rikz/Documents/Master/Semester2/SDDM/data/tmp') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "#         .config('spark.local.dir', '/data/s1847503/SDDM/tmp') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|           question0|                   -|                   -|How does temperat...|\n",
      "|  1|           question1|                   -|                   -|Seasonality of tr...|\n",
      "|  2|           question2|                   -|                   -|Effectiveness of ...|\n",
      "|  3|           question3|                   -|                   -|Effectiveness of ...|\n",
      "|  4|           question4|                   -|                   -|Effectiveness of ...|\n",
      "|  5|           question5|                   -|                   -|Effectiveness of ...|\n",
      "|  6|           question6|                   -|                   -|Effectiveness of ...|\n",
      "|  7|           question7|                   -|                   -|Effectiveness of ...|\n",
      "|  8|           question8|                   -|                   -|Significant chang...|\n",
      "|  9|           question9|                   -|                   -|Effectiveness of ...|\n",
      "| 10|20ee844014e6b7c91...|Coronavirus disea...|['John P A Ioanni...|\"The evolving cor...|\n",
      "| 11|3d2e51a4d7e9699e4...|COVID-19 and mate...|                  []|tems to ensure fo...|\n",
      "| 12|dd4cba9cda9f49ba9...|Systematic review...|                  []|Coronavirus has k...|\n",
      "| 13|2ef0ac31fd85f28b5...|SARS-CoV-Encoded ...|['Lucía Morales',...|In Brief SARS-CoV...|\n",
      "| 14|4bae3f6031a50a23b...|Effect of HA330 r...|['Xuefeng Xu', 'C...|\"The acute respir...|\n",
      "| 15|150b0fed0020d4549...|Traditional Chine...|           ['Ke He']|\"Traditional medi...|\n",
      "| 16|0ed26fea4f7e1a99d...|A new coronavirus...|['Fan Wu', 'Su Zh...|a Amino acids of ...|\n",
      "| 17|4b9e5d0ffdac80fba...|Article history: ...|['Kyung Sook Jung...|The major communi...|\n",
      "| 18|41fadbfc4def2200b...|49 Intellectual P...|                  []|\"Use of biotechno...|\n",
      "| 19|a8a2882316256ca57...|Region-resolved p...|['Hao-Liang Hu', ...|especially mitral...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "        .load('/home/rikz/Documents/Master/Semester2/SDDM/data/data.csv')\n",
    "#       .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|publish_time|               title|                 doi|             journal|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|d1aafb70c066a2068...|  2001-07-04|Clinical features...|10.1186/1471-2334...|      BMC Infect Dis|\n",
      "|6b0567729c2143a66...|  2000-08-15|Nitric oxide: a p...|        10.1186/rr14|          Respir Res|\n",
      "|06ced00a5fc042159...|  2000-08-25|Surfactant protei...|        10.1186/rr19|          Respir Res|\n",
      "|348055649b6b8cf2b...|  2001-02-22|Role of endotheli...|        10.1186/rr44|          Respir Res|\n",
      "|5f48792a5fa08bed9...|  2001-05-11|Gene expression i...|        10.1186/rr61|          Respir Res|\n",
      "|b2897e1277f566411...|  2001-12-17|Sequence requirem...|10.1093/emboj/20....|    The EMBO Journal|\n",
      "|3bb07ea10432f7738...|  2001-03-08|Debate: Transfusi...|       10.1186/cc987|           Crit Care|\n",
      "|5806726a24dc91de3...|  2001-05-02|The 21st Internat...|      10.1186/cc1013|           Crit Care|\n",
      "|faaf1022ccfe93b03...|  2003-08-07|Heme oxygenase-1 ...|10.1186/1465-9921...|          Respir Res|\n",
      "|5b44feca5d6ffaaeb...|  2003-09-01|Technical Descrip...| 10.1197/jamia.m1345|Journal of the Am...|\n",
      "|9d4e3e8eb092d5ed2...|  2000-04-17|Conservation of p...|10.1093/emboj/19....|              EMBO J|\n",
      "|14e0cac6e86d62859...|  2000-09-01|Heterogeneous nuc...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|d09b79026117ec9fa...|  2003-12-12|A Method to Ident...|       10.1251/bpo66|  Biol Proced Online|\n",
      "|44102e3e69e70ad2a...|  2000-08-01|Vaccinia virus in...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|6e8517cb25ff228cb...|  2004-01-20|The site of origi...|10.1186/1479-5876...|        J Transl Med|\n",
      "|30a4842a2e257f725...|  2004-05-26|Multi-faceted, mu...|10.1186/1742-4690...|       Retrovirology|\n",
      "|6a8ac55ea2a1fbd99...|  2004-03-31|Herpes simplex vi...|      10.1186/cc2850|           Crit Care|\n",
      "|367af6bb9a8bbda02...|  2004-08-06|Logistics of comm...|10.1186/1471-2458...|   BMC Public Health|\n",
      "|4df2c6eecb985fcb2...|  2004-09-27|Protection of pul...|10.1186/1465-9921...|          Respir Res|\n",
      "|83b05e8afa6cbe7a6...|  2005-01-03|Bioinformatic map...|10.1186/1471-2164...|        BMC Genomics|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_metadata = sqlContext.read.format('csv').options(header='true') \\\n",
    "                .load('/home/rikz/Documents/Master/Semester2/SDDM/data/metadata.csv') \\\n",
    "                .select(col('sha').alias('paper_id'), 'publish_time', 'title', 'doi', 'journal')\n",
    "\n",
    "df_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|question_id|           full_text|        preprocessed|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          0|How does temperat...|[temperature, hum...|\n",
      "|          1|Seasonality of tr...|[seasonality, tra...|\n",
      "|          2|Effectiveness of ...|[effectiveness, i...|\n",
      "|          3|Effectiveness of ...|[effectiveness, p...|\n",
      "|          4|Effectiveness of ...|[effectiveness, s...|\n",
      "|          5|Effectiveness of ...|[effectiveness, c...|\n",
      "|          6|Effectiveness of ...|[effectiveness, m...|\n",
      "|          7|Effectiveness of ...|[effectiveness, c...|\n",
      "|          8|Significant chang...|[change, transmis...|\n",
      "|          9|Effectiveness of ...|[effectiveness, w...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions = sqlContext.read.format('csv').options(header='true').load('/home/rikz/Documents/Master/Semester2/SDDM/data/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', 'full_text', col('finished_clean_lemma').alias('preprocessed'))\n",
    "questions_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Effectiveness of inter inner travel restriction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the question from 0 to 9\n",
    "question_num = 2\n",
    "\n",
    "questions_clean = questions_clean.filter(questions_clean.question_id == question_num)\n",
    "q = questions_clean.first().full_text\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers and duplicates: 510 rows.\n",
      "Removed empty papers\n",
      "Removed duplicates\n",
      "After removing empty papers and duplicates: 509 rows.\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| id|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| 54|86c3b8562c3094e95...|Title: A Feminist...|\"Natural disaster...|        498|[natural, disaste...|\n",
      "| 56|55743de644f6eda86...|Anti-microbial im...|Chronic obstructi...|       3601|[chronic, obstruc...|\n",
      "|100|690178e12695c5fdc...|                null|As of this date, ...|       1668|[case, covid, rep...|\n",
      "| 79|5009e8203ed264e9d...|Coronavirus Rever...|\"1 Introduction T...|        316|[introduction, ta...|\n",
      "|278|7f7940bc0ca1e1986...|Update in the tre...|Each year respira...|       7145|[year, respirator...|\n",
      "|396|0d76b14e9ae608525...|Viral Life Cycles...|\"Electron microsc...|        437|[electron, micros...|\n",
      "|234|86e5d3988edd1aa3d...|Potent Antiviral ...|\"The COVID-19 out...|         91|[covid, outbreak,...|\n",
      "|205|3c26c921c42bb4b2c...|Viral Infections ...|\"This article is ...|        300|[article, protect...|\n",
      "| 92|be05821eac40596ed...|Structural charac...|Emerging infectio...|       6360|[emerge, infectio...|\n",
      "|423|bdd32f74cc1be5920...|Anti-In£uenzaViru...|The replicative c...|       7366|[replicative, cyc...|\n",
      "|495|263f12984a135507e...|Asymmetric effect...|\"A growing body o...|       1032|[grow, body, argu...|\n",
      "|474|9448084d08566af74...|Tropical Medicine...|\"The global disco...|       4480|[global, discover...|\n",
      "| 13|2ef0ac31fd85f28b5...|SARS-CoV-Encoded ...|In Brief SARS-CoV...|       6708|[sarscov, exacerb...|\n",
      "|294|b4208007d70786df3...|Initial stage of ...|There are various...|        693|[scenario, model,...|\n",
      "|319|1af8c29bad3166064...|                null|explained that in...|       1263|[explain, growth,...|\n",
      "|212|42ce4e5eac6e69f5d...|The development o...|\"Infectious conju...|       1053|[infectious, conj...|\n",
      "|419|fc7c462f4b3c1a7e4...|Human Coronavirus...|\"T he recent iden...|       1306|[identification, ...|\n",
      "|362|5c3bfe9b503d23810...|Potential anti-SA...|A novel coronavir...|       2292|[coronavirus, sev...|\n",
      "| 69|22264c65eb6c14ba9...|Quality control i...|Although the pote...|       1439|[potential, mngs,...|\n",
      "|203|57b1179e96db85fd1...|A novel intracell...|Bovine viral diar...|       5785|[bovine, viral, d...|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "print(\"Before removing empty papers and duplicates: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "print(\"Removed empty papers\")\n",
    "df = df.dropDuplicates(subset=['full_text'])\n",
    "print(\"Removed duplicates\")\n",
    "print(\"After removing empty papers and duplicates: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                col('_c0').alias('id'),\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = []\n",
    "tf_idf_papers = []\n",
    "\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = []\n",
    "tf_idf_questions = []\n",
    "\n",
    "tf_q = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_papers.show()\n",
    "tf_idf_questions = tf_idf_questions.select('question_id', 'feature')\n",
    "tf_idf_papers = tf_idf_papers.select('id', 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions \\\n",
    "                    .select('question_id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers \\\n",
    "                    .select('id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = matrix_p.multiply(matrix_q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          similarity|\n",
      "+---+--------------------+\n",
      "|304| 0.24827753653021215|\n",
      "|206|  0.0861914701302207|\n",
      "|190| 0.07312866590439197|\n",
      "|163|0.042204374243260706|\n",
      "|201| 0.04028920227696964|\n",
      "|409| 0.03925788839039124|\n",
      "|457| 0.03658611602372874|\n",
      "|147| 0.03639500048794205|\n",
      "|489| 0.03489788271707602|\n",
      "|383| 0.03255684965496888|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = sc.parallelize(sim_matrix[:, question_num].tolist()) \\\n",
    "                .zipWithIndex() \\\n",
    "                .toDF(['similarity', 'id'])\n",
    "\n",
    "# Remove questions from the paper list\n",
    "# Sort on cosine similarity\n",
    "# Take the top 10 relevant documents\n",
    "relevant = relevant.select('id', 'similarity') \\\n",
    "                .filter(relevant.id > 9) \\\n",
    "                .sort(col('similarity').desc()) \\\n",
    "                .limit(10)\n",
    "\n",
    "relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Effectiveness of inter inner travel restriction\n",
      "\n",
      "Relevant Papers:\n",
      "\n",
      "+--------------------+--------------------+\n",
      "|            paper_id|          similarity|\n",
      "+--------------------+--------------------+\n",
      "|2d1300359d8ec1682...| 0.24827753653021215|\n",
      "|4be9882705e5d0021...|  0.0861914701302207|\n",
      "|a8b5de09c002605e9...| 0.07312866590439197|\n",
      "|9735c289f323e4ede...|0.042204374243260706|\n",
      "|8645437ad8a6f8538...| 0.04028920227696964|\n",
      "|a1bbca4d6a52f9747...| 0.03925788839039124|\n",
      "|ce707aeb4fe129ed6...| 0.03658611602372874|\n",
      "|dc7726fbace94aca0...| 0.03639500048794205|\n",
      "|2307ca38237d9b328...| 0.03489788271707602|\n",
      "|cda1f45974f3f33fa...| 0.03255684965496888|\n",
      "+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data of the 10 most relevant papers in order of relevance\n",
    "relevant_ids = [int(row.id) for row in relevant.collect()]\n",
    "print(\"Query: {}\".format(q))\n",
    "print()\n",
    "print(\"Relevant Papers:\")\n",
    "print()\n",
    "df_relevant = relevant.join(df.filter(df.id.isin(relevant_ids)), on=['id'], how='left_outer') \\\n",
    "                        .select('paper_id', 'similarity')\n",
    "df_relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>title</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2d1300359d8ec1682b0b6bdb51df4604a7ed4930</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>Epidemiological and clinical profile of Korean...</td>\n",
       "      <td>10.1097/md.0000000000017330</td>\n",
       "      <td>Medicine (Baltimore)</td>\n",
       "      <td>0.248278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4be9882705e5d0021b60347ea0ac3e2ae6d91b40</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>Development of the WHO-INTEGRATE evidence-to-d...</td>\n",
       "      <td>10.1186/s12962-020-0203-6</td>\n",
       "      <td>Cost Eff Resour Alloc</td>\n",
       "      <td>0.086191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a8b5de09c002605e9748e653b5ca827a23e83cf3</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>Calculating the potential for within-flight tr...</td>\n",
       "      <td>10.1186/1741-7015-7-81</td>\n",
       "      <td>BMC Med</td>\n",
       "      <td>0.073129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9735c289f323e4edefb7532889f9fa56d35a52cf</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Geriatric mental health and COVID-19: An eye-o...</td>\n",
       "      <td>10.1016/j.jagp.2020.05.009</td>\n",
       "      <td>Am J Geriatr Psychiatry</td>\n",
       "      <td>0.042204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8645437ad8a6f8538447b79ff2652174d9aba0d0</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>Accessibility and site suitability for healthc...</td>\n",
       "      <td>10.1007/s41324-020-00330-0</td>\n",
       "      <td>Spat</td>\n",
       "      <td>0.040289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a1bbca4d6a52f9747ea0159d11b5493da1f0b29c</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>Psychiatry hospital management facing COVID-19...</td>\n",
       "      <td>10.1016/j.bbi.2020.04.018</td>\n",
       "      <td>Brain Behav Immun</td>\n",
       "      <td>0.039258</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>ce707aeb4fe129ed6e3b011c2776f48b9cb200d6</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.036586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dc7726fbace94aca0f29a112cafe36198734b13a</td>\n",
       "      <td>2020-04-26</td>\n",
       "      <td>Analysis of Effectiveness of Quarantine Measur...</td>\n",
       "      <td>10.1101/2020.04.21.20074245</td>\n",
       "      <td>None</td>\n",
       "      <td>0.036395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307ca38237d9b328510976a5cf6ac8dbd0a733a</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>Preparing for uncertainty during public health...</td>\n",
       "      <td>10.1177/0840470420917172</td>\n",
       "      <td>Healthc Manage Forum</td>\n",
       "      <td>0.034898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cda1f45974f3f33fa7ebb605b4a6618adf187599</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Martini‐Klinik experience on prostate cancer s...</td>\n",
       "      <td>10.1111/bju.15115</td>\n",
       "      <td>BJU Int</td>\n",
       "      <td>0.032557</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id publish_time  \\\n",
       "9  2d1300359d8ec1682b0b6bdb51df4604a7ed4930   2019-09-27   \n",
       "2  4be9882705e5d0021b60347ea0ac3e2ae6d91b40   2020-02-11   \n",
       "1  a8b5de09c002605e9748e653b5ca827a23e83cf3   2009-12-24   \n",
       "8  9735c289f323e4edefb7532889f9fa56d35a52cf   2020-05-18   \n",
       "6  8645437ad8a6f8538447b79ff2652174d9aba0d0   2020-05-11   \n",
       "4  a1bbca4d6a52f9747ea0159d11b5493da1f0b29c   2020-04-10   \n",
       "3  ce707aeb4fe129ed6e3b011c2776f48b9cb200d6         None   \n",
       "5  dc7726fbace94aca0f29a112cafe36198734b13a   2020-04-26   \n",
       "0  2307ca38237d9b328510976a5cf6ac8dbd0a733a   2020-03-31   \n",
       "7  cda1f45974f3f33fa7ebb605b4a6618adf187599   2020-05-18   \n",
       "\n",
       "                                               title  \\\n",
       "9  Epidemiological and clinical profile of Korean...   \n",
       "2  Development of the WHO-INTEGRATE evidence-to-d...   \n",
       "1  Calculating the potential for within-flight tr...   \n",
       "8  Geriatric mental health and COVID-19: An eye-o...   \n",
       "6  Accessibility and site suitability for healthc...   \n",
       "4  Psychiatry hospital management facing COVID-19...   \n",
       "3                                               None   \n",
       "5  Analysis of Effectiveness of Quarantine Measur...   \n",
       "0  Preparing for uncertainty during public health...   \n",
       "7  Martini‐Klinik experience on prostate cancer s...   \n",
       "\n",
       "                           doi                  journal  similarity  \n",
       "9  10.1097/md.0000000000017330     Medicine (Baltimore)    0.248278  \n",
       "2    10.1186/s12962-020-0203-6    Cost Eff Resour Alloc    0.086191  \n",
       "1       10.1186/1741-7015-7-81                  BMC Med    0.073129  \n",
       "8   10.1016/j.jagp.2020.05.009  Am J Geriatr Psychiatry    0.042204  \n",
       "6   10.1007/s41324-020-00330-0                     Spat    0.040289  \n",
       "4    10.1016/j.bbi.2020.04.018        Brain Behav Immun    0.039258  \n",
       "3                         None                     None    0.036586  \n",
       "5  10.1101/2020.04.21.20074245                     None    0.036395  \n",
       "0     10.1177/0840470420917172     Healthc Manage Forum    0.034898  \n",
       "7            10.1111/bju.15115                  BJU Int    0.032557  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the summary table with the relevant paper from the metadata\n",
    "df_relevant = df_relevant.join(df_metadata, on=['paper_id'], how='left_outer') \\\n",
    "                            .select('paper_id', 'publish_time', 'title', 'doi', 'journal', 'similarity') \\\n",
    "                            .toPandas() \\\n",
    "                            .sort_values(by='similarity', ascending=False)\n",
    "df_relevant.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table extracted and sent to csv file.\n"
     ]
    }
   ],
   "source": [
    "# Send the summary table to a csv file\n",
    "df_relevant.to_csv('/home/rikz/Documents/Master/Semester2/SDDM/SDDM/summary_tables/{}.csv' \\\n",
    "                   .format(q.lower().replace(' ', '_')), index=False)\n",
    "print(\"Summary table extracted and sent to csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(inputCol='preprocessed', outputCol='word_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2Vec.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_clean = model.transform(questions_clean)\n",
    "ques_vec = questions_clean.first().word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between a document vector and a question vector\n",
    "def cossim(doc_vec): \n",
    "    global ques_vec\n",
    "    sim = np.dot(doc_vec, ques_vec) / np.sqrt(np.dot(doc_vec, ques_vec)) / np.sqrt(np.dot(doc_vec, ques_vec)) \n",
    "    return float(sim)\n",
    "\n",
    "cossim_udf = udf(cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('id', cossim_udf('word_vector').alias('similarity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"Before removing empty papers: {} rows.\".format(df2.count()))\n",
    "print(\"x\")\n",
    "df2 = df2.filter(df2.similarity.isNotNull())\n",
    "print(\"After removing empty papers 1: {} rows.\".format(df2.count()))\n",
    "df2 = df2.filter(df2.similarity != 'NaN')\n",
    "# df2 = df2.dropna(subset='similarity')\n",
    "print(\"After removing empty papers 2: {} rows.\".format(df2.count()))\n",
    "df2 = df2.orderBy('similarity', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relevant = [40821, 40931, 40823, 40776, 40831]\n",
    "for r in relevant:\n",
    "    print(df.filter(df.id == 40821).first().title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
