{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training information retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, explode, col, countDistinct, collect_list\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '200g') \\\n",
    "        .config('spark.executor.memory', '200g') \\\n",
    "        .config('spark.executor.cores', '32') \\\n",
    "        .config('spark.memory.fraction', '0.8') \\\n",
    "        .config('spark.local.dir', '/data/s1847503/SDDM/tmp') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "# .config('spark.memory.fraction', '0.8') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|df97f804b68dcf16f...|Articles The effe...|['Kiesha Prem', '...|\"Severe acute res...| respectively. Ho...|\n",
      "|  1|0475f4122241f4008...|Impact of Social ...|['Xutong Wang', '...|75% or 90%. We co...|\"['75% or 90%. We...|\n",
      "|  2|94586be17a5f9eca8...|Lower State COVID...|  ['Emily Rauscher']|Evidence is mixed...|['Evidence is mix...|\n",
      "|  3|           question0|                   -|                   -|Effectiveness of ...|                   -|\n",
      "|  4|           question1|                   -|                   -|Methods to unders...|                   -|\n",
      "|  5|           question2|                   -|                   -|Evidence that dom...|                   -|\n",
      "|  6|           question3|                   -|                   -|Effectiveness of ...|                   -|\n",
      "|  7|           question4|                   -|                   -|Effectiveness of ...|                   -|\n",
      "|  8|           question5|                   -|                   -|Effectiveness of ...|                   -|\n",
      "|  9|           question6|                   -|                   -|Effectiveness of ...|                   -|\n",
      "| 10|           question7|                   -|                   -|Effectiveness of ...|                   -|\n",
      "| 11|           question8|                   -|                   -|Effectiveness of ...|                   -|\n",
      "| 12|           question9|                   -|                   -|Seasonality of tr...|                   -|\n",
      "| 13|          question10|                   -|                   -|How does temperat...|                   -|\n",
      "| 14|          question11|                   -|                   -|What is the likel...|                   -|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "      .load('/data/s1847503/SDDM/newdata/relevant.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# word_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
    "#                           .setInputCols([\"document\", \"clean_lemma\"]) \\\n",
    "#                           .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])\n",
    "\n",
    "# bert_pipeline = Pipeline() \\\n",
    "#                     .setStages([\n",
    "#                         document_assembler,\n",
    "#                         tokenizer,\n",
    "#                         normalizer,\n",
    "#                         lemmatizer,\n",
    "#                         stopwords_cleaner,\n",
    "#                         word_embeddings\n",
    "#                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['effectiveness',\n",
       " 'multifactorial',\n",
       " 'strategy',\n",
       " 'prevent',\n",
       " 'secondary',\n",
       " 'transmission']"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the question from 0 to 11\n",
    "question_num = 7\n",
    "\n",
    "questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', 'full_text', col('finished_clean_lemma').alias('preprocessed'))\n",
    "questions_clean = questions_clean.filter(questions_clean.question_id > question_num)\n",
    "question_nums = [int(n) for n in questions_clean.select(collect_list('question_id')).first()[0]]\n",
    "questions_clean.first().preprocessed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers: 15 rows.\n",
      "After removing empty papers: 15 rows.\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| id|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|  0|df97f804b68dcf16f...|Articles The effe...|\"Severe acute res...|        938|[severe, acute, r...|\n",
      "|  1|0475f4122241f4008...|Impact of Social ...|75% or 90%. We co...|       4369|[compare, early, ...|\n",
      "|  2|94586be17a5f9eca8...|Lower State COVID...|Evidence is mixed...|        703|[evidence, mix, e...|\n",
      "|  3|           question0|                   -|Effectiveness of ...|          6|[effectiveness, i...|\n",
      "|  4|           question1|                   -|Methods to unders...|          9|[method, understa...|\n",
      "|  5|           question2|                   -|Evidence that dom...|         14|[evidence, domest...|\n",
      "|  6|           question3|                   -|Effectiveness of ...|          4|[effectiveness, s...|\n",
      "|  7|           question4|                   -|Effectiveness of ...|          8|[effectiveness, w...|\n",
      "|  8|           question5|                   -|Effectiveness of ...|          5|[effectiveness, c...|\n",
      "|  9|           question6|                   -|Effectiveness of ...|         12|[effectiveness, c...|\n",
      "| 10|           question7|                   -|Effectiveness of ...|          8|[effectiveness, p...|\n",
      "| 11|           question8|                   -|Effectiveness of ...|          9|[effectiveness, m...|\n",
      "| 12|           question9|                   -|Seasonality of tr...|          3|[seasonality, tra...|\n",
      "| 13|          question10|                   -|How does temperat...|         10|[temperature, hum...|\n",
      "| 14|          question11|                   -|What is the likel...|         12|[likelihood, chan...|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "# Keep only papers with a text length of greater than 10\n",
    "print(\"Before removing empty papers: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "# df = df.dropduplicates(subset='title')\n",
    "# print(\"Removed duplicates\")\n",
    "# df = df.filter(df['text_length'] > 10)\n",
    "print(\"After removing empty papers: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                col('_c0').alias('id'),\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = []\n",
    "tf_idf_papers = []\n",
    "\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = []\n",
    "tf_idf_questions = []\n",
    "\n",
    "tf_q = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_papers.show()\n",
    "tf_idf_questions = tf_idf_questions.drop('norm')\n",
    "tf_idf_papers = tf_idf_papers.drop('norm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes to csv files\n",
    "# tf_idf_papers.select('id', 'norm').write.csv('/data/s1847503/SDDM/newdata/tfidf_papers.csv')\n",
    "# tf_idf_questions.select('id', 'norm').write.csv('/data/s1847503/SDDM/newdata/tfidf_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions.select('question_id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers.select('id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = matrix_p.multiply(matrix_q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.007647</td>\n",
       "      <td>0.004753</td>\n",
       "      <td>0.018852</td>\n",
       "      <td>0.067077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.022548</td>\n",
       "      <td>0.011434</td>\n",
       "      <td>0.022214</td>\n",
       "      <td>0.004014</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.001634</td>\n",
       "      <td>0.002492</td>\n",
       "      <td>0.001452</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.054711</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.132749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.081821</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.400923</td>\n",
       "      <td>0.054761</td>\n",
       "      <td>0.031917</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.061298</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.245242</td>\n",
       "      <td>0.033497</td>\n",
       "      <td>0.019524</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.034640</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.046642</td>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.046642</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.027185</td>\n",
       "      <td>0.041449</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           8         9        10        11\n",
       "0   0.007647  0.004753  0.018852  0.067077\n",
       "1   0.022548  0.011434  0.022214  0.004014\n",
       "2   0.001634  0.002492  0.001452  0.000000\n",
       "3   0.054711  0.000000  0.000000  0.000000\n",
       "4   0.000000  0.000000  0.000000  0.000000\n",
       "5   0.000000  0.000000  0.000000  0.132749\n",
       "6   0.081821  0.000000  0.000000  0.000000\n",
       "7   0.400923  0.054761  0.031917  0.000000\n",
       "8   0.061298  0.000000  0.000000  0.000000\n",
       "9   0.245242  0.033497  0.019524  0.000000\n",
       "10  0.034640  0.000000  0.000000  0.000000\n",
       "11  1.000000  0.046642  0.027185  0.000000\n",
       "12  0.046642  1.000000  0.041449  0.000000\n",
       "13  0.027185  0.041449  1.000000  0.000000\n",
       "14  0.000000  0.000000  0.000000  1.000000"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "d = {}\n",
    "for n in question_nums:\n",
    "    d[str(n)] = sim_matrix[:, n]\n",
    "sim = pd.DataFrame(d)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|df97f804b68dcf16f...|Articles The effe...|['Kiesha Prem', '...|\"Severe acute res...| respectively. Ho...|\n",
      "|  1|0475f4122241f4008...|Impact of Social ...|['Xutong Wang', '...|75% or 90%. We co...|\"['75% or 90%. We...|\n",
      "|  2|94586be17a5f9eca8...|Lower State COVID...|  ['Emily Rauscher']|Evidence is mixed...|['Evidence is mix...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rel = sqlContext.read.format('csv').options(header='true') \\\n",
    "      .load('/data/s1847503/SDDM/newdata/relevant.csv')\n",
    "df_rel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study\n",
      "\n",
      "Impact of Social Distancing Measures on COVID-19 Healthcare Demand in Central Texas\n",
      "\n",
      "Lower State COVID-19 Deaths and Cases with Earlier School Closure in the U.S\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = [0, 1, 2]\n",
    "for r in relevant:\n",
    "    print(df_rel.filter(df_rel._c0 == r).first().title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(inputCol='preprocessed', outputCol='word_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2Vec.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "| id|               title|           full_text|text_length|        preprocessed|         word_vector|\n",
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "|  0|Generation of pre...|\"The infection of...|        723|[infection, newly...|[0.04855843337612...|\n",
      "|  1|Zoonotic disease ...|\"Veterinary profe...|       1756|[veterinary, prof...|[-0.0769096092344...|\n",
      "|  2|Current and Novel...|\"Influenza viruse...|        919|[influenza, virus...|[0.06106203970513...|\n",
      "|  3|MERS: Progress on...|Since its identif...|       3942|[identification, ...|[-0.1404365635089...|\n",
      "|  4|Hepatologie Akute...|\"Das akute Leberv...|       1832|[akute, lebervers...|[9.99946369923657...|\n",
      "|  5|Novel approach to...|\"Introduction | T...|        448|[introduction, qu...|[0.05870716313081...|\n",
      "|  6|On the electrific...|Scientists, polic...|        880|[scientist, polic...|[-0.0176009940438...|\n",
      "|  7|A dynamic model f...|Infectious diseas...|       7535|[infectious, dise...|[0.09400318366228...|\n",
      "|  8|Critical evaluati...|Respiratory multi...|       7187|[respiratory, mul...|[-0.0579856613914...|\n",
      "|  9|The influence of ...|Airborne transmis...|       5729|[airborne, transm...|[-0.0324264039747...|\n",
      "| 10|Association betwe...|events cannot be ...|        883|[event, observe, ...|[0.08607622502086...|\n",
      "| 11|Structural insigh...|Coronavirus is a ...|       3517|[coronavirus, pos...|[0.09218984661542...|\n",
      "| 12|Redistributing wo...|\"institutions dea...|        242|[institution, dea...|[-0.1008086456225...|\n",
      "| 13|LY6E impairs coro...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|[0.00507427492838...|\n",
      "| 14|M2-Polarized Macr...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|[-0.0342680080975...|\n",
      "| 15|Open Access SHORT...|In March and Apri...|       1801|[march, april, sw...|[-0.0402347918031...|\n",
      "| 16|Mating strategy i...|across species, w...|        917|[species, number,...|[-0.0641917856157...|\n",
      "| 17|What are the main...|Healthcare-associ...|       1737|[healthcareassoci...|[-0.0851379528752...|\n",
      "| 18|Going to Bat(s) f...|\"An estimated ∼60...|       1673|[estimate, emerge...|[-0.1511569294494...|\n",
      "| 19|Detection and cha...|family, was first...|       2773|[family, detect, ...|[-0.0682785567394...|\n",
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_clean = model.transform(questions_clean)\n",
    "ques_vec = questions_clean.first().word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between a document vector and a question vector\n",
    "def cossim(doc_vec): \n",
    "    global ques_vec\n",
    "    sim = np.dot(doc_vec, ques_vec) / np.sqrt(np.dot(doc_vec, ques_vec)) / np.sqrt(np.dot(doc_vec, ques_vec)) \n",
    "    return float(sim)\n",
    "\n",
    "cossim_udf = udf(cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('id', cossim_udf('word_vector').alias('similarity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        similarity|\n",
      "+---+------------------+\n",
      "|  0|0.9999999999999998|\n",
      "|  1|               1.0|\n",
      "|  2|               1.0|\n",
      "|  3|1.0000000000000002|\n",
      "|  4|               NaN|\n",
      "|  5|               1.0|\n",
      "|  6|0.9999999999999999|\n",
      "|  7|0.9999999999999999|\n",
      "|  8|               1.0|\n",
      "|  9|               1.0|\n",
      "| 10|1.0000000000000002|\n",
      "| 11|0.9999999999999998|\n",
      "| 12|1.0000000000000002|\n",
      "| 13|               NaN|\n",
      "| 14|1.0000000000000002|\n",
      "| 15|0.9999999999999999|\n",
      "| 16|               1.0|\n",
      "| 17|1.0000000000000002|\n",
      "| 18|1.0000000000000002|\n",
      "| 19|1.0000000000000002|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "After removing empty papers 1: 59561 rows.\n",
      "After removing empty papers 2: 53611 rows.\n",
      "+-----+------------------+\n",
      "|   id|        similarity|\n",
      "+-----+------------------+\n",
      "|40821|1.0000000000000002|\n",
      "|40931|1.0000000000000002|\n",
      "|40823|1.0000000000000002|\n",
      "|40776|1.0000000000000002|\n",
      "|40831|1.0000000000000002|\n",
      "|40784|1.0000000000000002|\n",
      "|40832|1.0000000000000002|\n",
      "|40792|1.0000000000000002|\n",
      "|40843|1.0000000000000002|\n",
      "|40803|1.0000000000000002|\n",
      "|40851|1.0000000000000002|\n",
      "|40813|1.0000000000000002|\n",
      "|40854|1.0000000000000002|\n",
      "|40816|1.0000000000000002|\n",
      "|40862|1.0000000000000002|\n",
      "|40870|1.0000000000000002|\n",
      "|40875|1.0000000000000002|\n",
      "|40774|1.0000000000000002|\n",
      "|40877|1.0000000000000002|\n",
      "|40785|1.0000000000000002|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Before removing empty papers: {} rows.\".format(df2.count()))\n",
    "print(\"x\")\n",
    "df2 = df2.filter(df2.similarity.isNotNull())\n",
    "print(\"After removing empty papers 1: {} rows.\".format(df2.count()))\n",
    "df2 = df2.filter(df2.similarity != 'NaN')\n",
    "# df2 = df2.dropna(subset='similarity')\n",
    "print(\"After removing empty papers 2: {} rows.\".format(df2.count()))\n",
    "df2 = df2.orderBy('similarity', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = [40821, 40931, 40823, 40776, 40831]\n",
    "for r in relevant:\n",
    "    print(df.filter(df.id == 40821).first().title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
