{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training information retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, explode, col, countDistinct\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '200g') \\\n",
    "        .config('spark.executor.memory', '200g') \\\n",
    "        .config('spark.executor.cores', '32') \\\n",
    "        .config('spark.memory.fraction', '0.8') \\\n",
    "        .config('spark.local.dir', '/data/s1847503/SDDM/tmp') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "# .config('spark.memory.fraction', '0.8') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|1329bb2f949e74925...|Generation of pre...|['Xue Wu Zhang', ...|\"The infection of...| 30 drugs were se...|\n",
      "|  1|dc079a2e9cf98fad0...|Zoonotic disease ...|['Charlotte Robin...|\"Veterinary profe...| based on the par...|\n",
      "|  2|75af9aa0e63889abd...|Current and Novel...|['Erasmus Kotey',...|\"Influenza viruse...| although LAIVs a...|\n",
      "|  3|1755c4785f87bca19...|MERS: Progress on...|['*', 'Ryan Aguan...|Since its identif...|['Since its ident...|\n",
      "|  4|cc829c0f2ab2e110b...|Hepatologie Akute...|['Karoline Rutter...|\"Das akute Leberv...| nach Ausschluss ...|\n",
      "|  5|ece3d68d9b996c917...|Novel approach to...|['Ivan Timokhin',...|\"Introduction | T...|      diameter 12 mm|\n",
      "|  6|9cd0f74020b0db181...|On the electrific...|['Martin Weiss', ...|Scientists, polic...|        ''pedelecs\"\"|\n",
      "|  7|0b70c1fd82bd1962a...|A dynamic model f...|['P Raja', 'Sekha...|Infectious diseas...|\"['Infectious dis...|\n",
      "|  8|94e8acc14db64cbb1...|Critical evaluati...|['John D Diaz-Dec...|Respiratory multi...|\"[\"\"Respiratory m...|\n",
      "|  9|d4b11ed79efbb3cd5...|The influence of ...|['Jian Hang', 'Yu...|Airborne transmis...|\"['Airborne trans...|\n",
      "| 10|68a2a48d4c67318b0...|Association betwe...|                  []|events cannot be ...|['events cannot b...|\n",
      "| 11|0ccdc351858fd7dfe...|Structural insigh...|['Manish Sarkar',...|Coronavirus is a ...|\"['Coronavirus is...|\n",
      "| 12|81059d5922e947ca8...|Redistributing wo...|['Marko Ćurković'...|\"institutions dea...|       exceptionally|\n",
      "| 13|d23c6a066a58dbe5e...|LY6E impairs coro...|['Stephanie Pfaen...|\"4 hepatoma cells...| 16 . Pharmacolog...|\n",
      "| 14|42a6fd4f30e6c37c8...|M2-Polarized Macr...|['Tania Cristina'...|Lacaziosis is a c...|\"['Lacaziosis is ...|\n",
      "| 15|63c9d5537c05b45dd...|Open Access SHORT...|['Xiao-Ping Kang'...|In March and Apri...|\"['In March and A...|\n",
      "| 16|618cd102ec5051a05...|Mating strategy i...|['Federica Rosset...|across species, w...|  \"[\"\"across species|\n",
      "| 17|4ae8c9c942c792ce0...|What are the main...|['M C Padoveze', ...|Healthcare-associ...|\"[\"\"Healthcare-as...|\n",
      "| 18|5af166022c0575cef...|Going to Bat(s) f...|['Irah L King', '...|\"An estimated ∼60...| 44) . Although t...|\n",
      "| 19|c76e4f5711ab12d65...|Detection and cha...|['Toshihiro Ito',...|family, was first...|\"['family, was fi...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "      .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# word_embeddings = BertEmbeddings.pretrained('bert_base_cased', 'en') \\\n",
    "#                           .setInputCols([\"document\", \"clean_lemma\"]) \\\n",
    "#                           .setOutputCol(\"embeddings\")\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])\n",
    "\n",
    "# bert_pipeline = Pipeline() \\\n",
    "#                     .setStages([\n",
    "#                         document_assembler,\n",
    "#                         tokenizer,\n",
    "#                         normalizer,\n",
    "#                         lemmatizer,\n",
    "#                         stopwords_cleaner,\n",
    "#                         word_embeddings\n",
    "#                      ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Effectiveness of school distancing'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select the question from 0 to 11\n",
    "question_num = 3\n",
    "\n",
    "questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', 'full_text', col('finished_clean_lemma').alias('preprocessed'))\n",
    "questions_clean = questions_clean.filter(questions_clean.question_id == question_num)\n",
    "questions_clean.first().full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers: 59561 rows.\n",
      "After removing empty papers: 59561 rows.\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| id|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|  0|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|\n",
      "|  1|dc079a2e9cf98fad0...|Zoonotic disease ...|\"Veterinary profe...|       1756|[veterinary, prof...|\n",
      "|  2|75af9aa0e63889abd...|Current and Novel...|\"Influenza viruse...|        919|[influenza, virus...|\n",
      "|  3|1755c4785f87bca19...|MERS: Progress on...|Since its identif...|       3942|[identification, ...|\n",
      "|  4|cc829c0f2ab2e110b...|Hepatologie Akute...|\"Das akute Leberv...|       1832|[akute, lebervers...|\n",
      "|  5|ece3d68d9b996c917...|Novel approach to...|\"Introduction | T...|        448|[introduction, qu...|\n",
      "|  6|9cd0f74020b0db181...|On the electrific...|Scientists, polic...|        880|[scientist, polic...|\n",
      "|  7|0b70c1fd82bd1962a...|A dynamic model f...|Infectious diseas...|       7535|[infectious, dise...|\n",
      "|  8|94e8acc14db64cbb1...|Critical evaluati...|Respiratory multi...|       7187|[respiratory, mul...|\n",
      "|  9|d4b11ed79efbb3cd5...|The influence of ...|Airborne transmis...|       5729|[airborne, transm...|\n",
      "| 10|68a2a48d4c67318b0...|Association betwe...|events cannot be ...|        883|[event, observe, ...|\n",
      "| 11|0ccdc351858fd7dfe...|Structural insigh...|Coronavirus is a ...|       3517|[coronavirus, pos...|\n",
      "| 12|81059d5922e947ca8...|Redistributing wo...|\"institutions dea...|        242|[institution, dea...|\n",
      "| 13|d23c6a066a58dbe5e...|LY6E impairs coro...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|\n",
      "| 14|42a6fd4f30e6c37c8...|M2-Polarized Macr...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|\n",
      "| 15|63c9d5537c05b45dd...|Open Access SHORT...|In March and Apri...|       1801|[march, april, sw...|\n",
      "| 16|618cd102ec5051a05...|Mating strategy i...|across species, w...|        917|[species, number,...|\n",
      "| 17|4ae8c9c942c792ce0...|What are the main...|Healthcare-associ...|       1737|[healthcareassoci...|\n",
      "| 18|5af166022c0575cef...|Going to Bat(s) f...|\"An estimated ∼60...|       1673|[estimate, emerge...|\n",
      "| 19|c76e4f5711ab12d65...|Detection and cha...|family, was first...|       2773|[family, detect, ...|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "# Keep only papers with a text length of greater than 10\n",
    "print(\"Before removing empty papers: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "# df = df.dropduplicates(subset='title')\n",
    "# print(\"Removed duplicates\")\n",
    "# df = df.filter(df['text_length'] > 10)\n",
    "print(\"After removing empty papers: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                col('_c0').alias('id'),\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = HashingTF(inputCol='clean_question', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_q) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_q) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tf_idf_papers.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataframes to csv files\n",
    "# tf_idf_papers.select('id', 'norm').write.csv('/data/s1847503/SDDM/newdata/tfidf_papers.csv')\n",
    "# tf_idf_questions.select('id', 'norm').write.csv('/data/s1847503/SDDM/newdata/tfidf_questions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions.select('question_id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers.select('id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "matrix_q.numCols()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = matrix_p.multiply(matrix_q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sim_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "d = {}\n",
    "# d[str(question_num)] = sim_matrix[:, i]\n",
    "sim = pd.DataFrame(d)\n",
    "sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|df97f804b68dcf16f...|Articles The effe...|['Kiesha Prem', '...|\"Severe acute res...| respectively. Ho...|\n",
      "|  1|0475f4122241f4008...|Impact of Social ...|['Xutong Wang', '...|75% or 90%. We co...|\"['75% or 90%. We...|\n",
      "|  2|94586be17a5f9eca8...|Lower State COVID...|  ['Emily Rauscher']|Evidence is mixed...|['Evidence is mix...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_rel = sqlContext.read.format('csv').options(header='true') \\\n",
    "      .load('/data/s1847503/SDDM/newdata/relevant.csv')\n",
    "df_rel.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Articles The effect of control strategies to reduce social mixing on outcomes of the COVID-19 epidemic in Wuhan, China: a modelling study\n",
      "\n",
      "Impact of Social Distancing Measures on COVID-19 Healthcare Demand in Central Texas\n",
      "\n",
      "Lower State COVID-19 Deaths and Cases with Earlier School Closure in the U.S\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = [0, 1, 2]\n",
    "for r in relevant:\n",
    "    print(df_rel.filter(df_rel._c0 == r).first().title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Word2Vec\n",
    "word2Vec = Word2Vec(inputCol='preprocessed', outputCol='word_vector')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = word2Vec.fit(df)\n",
    "df = model.transform(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "| id|               title|           full_text|text_length|        preprocessed|         word_vector|\n",
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "|  0|Generation of pre...|\"The infection of...|        723|[infection, newly...|[0.04855843337612...|\n",
      "|  1|Zoonotic disease ...|\"Veterinary profe...|       1756|[veterinary, prof...|[-0.0769096092344...|\n",
      "|  2|Current and Novel...|\"Influenza viruse...|        919|[influenza, virus...|[0.06106203970513...|\n",
      "|  3|MERS: Progress on...|Since its identif...|       3942|[identification, ...|[-0.1404365635089...|\n",
      "|  4|Hepatologie Akute...|\"Das akute Leberv...|       1832|[akute, lebervers...|[9.99946369923657...|\n",
      "|  5|Novel approach to...|\"Introduction | T...|        448|[introduction, qu...|[0.05870716313081...|\n",
      "|  6|On the electrific...|Scientists, polic...|        880|[scientist, polic...|[-0.0176009940438...|\n",
      "|  7|A dynamic model f...|Infectious diseas...|       7535|[infectious, dise...|[0.09400318366228...|\n",
      "|  8|Critical evaluati...|Respiratory multi...|       7187|[respiratory, mul...|[-0.0579856613914...|\n",
      "|  9|The influence of ...|Airborne transmis...|       5729|[airborne, transm...|[-0.0324264039747...|\n",
      "| 10|Association betwe...|events cannot be ...|        883|[event, observe, ...|[0.08607622502086...|\n",
      "| 11|Structural insigh...|Coronavirus is a ...|       3517|[coronavirus, pos...|[0.09218984661542...|\n",
      "| 12|Redistributing wo...|\"institutions dea...|        242|[institution, dea...|[-0.1008086456225...|\n",
      "| 13|LY6E impairs coro...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|[0.00507427492838...|\n",
      "| 14|M2-Polarized Macr...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|[-0.0342680080975...|\n",
      "| 15|Open Access SHORT...|In March and Apri...|       1801|[march, april, sw...|[-0.0402347918031...|\n",
      "| 16|Mating strategy i...|across species, w...|        917|[species, number,...|[-0.0641917856157...|\n",
      "| 17|What are the main...|Healthcare-associ...|       1737|[healthcareassoci...|[-0.0851379528752...|\n",
      "| 18|Going to Bat(s) f...|\"An estimated ∼60...|       1673|[estimate, emerge...|[-0.1511569294494...|\n",
      "| 19|Detection and cha...|family, was first...|       2773|[family, detect, ...|[-0.0682785567394...|\n",
      "+---+--------------------+--------------------+-----------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions_clean = model.transform(questions_clean)\n",
    "ques_vec = questions_clean.first().word_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate cosine similarity between a document vector and a question vector\n",
    "def cossim(doc_vec): \n",
    "    global ques_vec\n",
    "    sim = np.dot(doc_vec, ques_vec) / np.sqrt(np.dot(doc_vec, ques_vec)) / np.sqrt(np.dot(doc_vec, ques_vec)) \n",
    "    return float(sim)\n",
    "\n",
    "cossim_udf = udf(cossim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.select('id', cossim_udf('word_vector').alias('similarity'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+------------------+\n",
      "| id|        similarity|\n",
      "+---+------------------+\n",
      "|  0|0.9999999999999998|\n",
      "|  1|               1.0|\n",
      "|  2|               1.0|\n",
      "|  3|1.0000000000000002|\n",
      "|  4|               NaN|\n",
      "|  5|               1.0|\n",
      "|  6|0.9999999999999999|\n",
      "|  7|0.9999999999999999|\n",
      "|  8|               1.0|\n",
      "|  9|               1.0|\n",
      "| 10|1.0000000000000002|\n",
      "| 11|0.9999999999999998|\n",
      "| 12|1.0000000000000002|\n",
      "| 13|               NaN|\n",
      "| 14|1.0000000000000002|\n",
      "| 15|0.9999999999999999|\n",
      "| 16|               1.0|\n",
      "| 17|1.0000000000000002|\n",
      "| 18|1.0000000000000002|\n",
      "| 19|1.0000000000000002|\n",
      "+---+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x\n",
      "After removing empty papers 1: 59561 rows.\n",
      "After removing empty papers 2: 53611 rows.\n",
      "+-----+------------------+\n",
      "|   id|        similarity|\n",
      "+-----+------------------+\n",
      "|40821|1.0000000000000002|\n",
      "|40931|1.0000000000000002|\n",
      "|40823|1.0000000000000002|\n",
      "|40776|1.0000000000000002|\n",
      "|40831|1.0000000000000002|\n",
      "|40784|1.0000000000000002|\n",
      "|40832|1.0000000000000002|\n",
      "|40792|1.0000000000000002|\n",
      "|40843|1.0000000000000002|\n",
      "|40803|1.0000000000000002|\n",
      "|40851|1.0000000000000002|\n",
      "|40813|1.0000000000000002|\n",
      "|40854|1.0000000000000002|\n",
      "|40816|1.0000000000000002|\n",
      "|40862|1.0000000000000002|\n",
      "|40870|1.0000000000000002|\n",
      "|40875|1.0000000000000002|\n",
      "|40774|1.0000000000000002|\n",
      "|40877|1.0000000000000002|\n",
      "|40785|1.0000000000000002|\n",
      "+-----+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print(\"Before removing empty papers: {} rows.\".format(df2.count()))\n",
    "print(\"x\")\n",
    "df2 = df2.filter(df2.similarity.isNotNull())\n",
    "print(\"After removing empty papers 1: {} rows.\".format(df2.count()))\n",
    "df2 = df2.filter(df2.similarity != 'NaN')\n",
    "# df2 = df2.dropna(subset='similarity')\n",
    "print(\"After removing empty papers 2: {} rows.\".format(df2.count()))\n",
    "df2 = df2.orderBy('similarity', ascending=False).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n",
      "Buried treasure: evolutionary perspectives on microbial iron piracy An evolving view of host-microbe interactions\n",
      "\n"
     ]
    }
   ],
   "source": [
    "relevant = [40821, 40931, 40823, 40776, 40831]\n",
    "for r in relevant:\n",
    "    print(df.filter(df.id == 40821).first().title)\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
