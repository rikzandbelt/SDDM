{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training information retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import size, explode, col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '64g') \\\n",
    "        .config('spark.executor.memory', '32g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "# .config('spark.memory.fraction', '0.8') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|['Xue Wu Zhang', ...|\"The infection of...| 30 drugs were se...|\n",
      "|dc079a2e9cf98fad0...|Zoonotic disease ...|['Charlotte Robin...|\"Veterinary profe...| based on the par...|\n",
      "|75af9aa0e63889abd...|Current and Novel...|['Erasmus Kotey',...|\"Influenza viruse...| although LAIVs a...|\n",
      "|1755c4785f87bca19...|MERS: Progress on...|['*', 'Ryan Aguan...|Since its identif...|['Since its ident...|\n",
      "|cc829c0f2ab2e110b...|Hepatologie Akute...|['Karoline Rutter...|\"Das akute Leberv...| nach Ausschluss ...|\n",
      "|ece3d68d9b996c917...|Novel approach to...|['Ivan Timokhin',...|\"Introduction | T...|      diameter 12 mm|\n",
      "|9cd0f74020b0db181...|On the electrific...|['Martin Weiss', ...|Scientists, polic...|        ''pedelecs\"\"|\n",
      "|0b70c1fd82bd1962a...|A dynamic model f...|['P Raja', 'Sekha...|Infectious diseas...|\"['Infectious dis...|\n",
      "|94e8acc14db64cbb1...|Critical evaluati...|['John D Diaz-Dec...|Respiratory multi...|\"[\"\"Respiratory m...|\n",
      "|d4b11ed79efbb3cd5...|The influence of ...|['Jian Hang', 'Yu...|Airborne transmis...|\"['Airborne trans...|\n",
      "|68a2a48d4c67318b0...|Association betwe...|                  []|events cannot be ...|['events cannot b...|\n",
      "|0ccdc351858fd7dfe...|Structural insigh...|['Manish Sarkar',...|Coronavirus is a ...|\"['Coronavirus is...|\n",
      "|81059d5922e947ca8...|Redistributing wo...|['Marko Ćurković'...|\"institutions dea...|       exceptionally|\n",
      "|d23c6a066a58dbe5e...|LY6E impairs coro...|['Stephanie Pfaen...|\"4 hepatoma cells...| 16 . Pharmacolog...|\n",
      "|42a6fd4f30e6c37c8...|M2-Polarized Macr...|['Tania Cristina'...|Lacaziosis is a c...|\"['Lacaziosis is ...|\n",
      "|63c9d5537c05b45dd...|Open Access SHORT...|['Xiao-Ping Kang'...|In March and Apri...|\"['In March and A...|\n",
      "|618cd102ec5051a05...|Mating strategy i...|['Federica Rosset...|across species, w...|  \"[\"\"across species|\n",
      "|4ae8c9c942c792ce0...|What are the main...|['M C Padoveze', ...|Healthcare-associ...|\"[\"\"Healthcare-as...|\n",
      "|5af166022c0575cef...|Going to Bat(s) f...|['Irah L King', '...|\"An estimated ∼60...| 44) . Although t...|\n",
      "|c76e4f5711ab12d65...|Detection and cha...|['Toshihiro Ito',...|family, was first...|\"['family, was fi...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "      .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Effectiveness of inter_inner travel restriction ', 'Methods to understand and regulate the spread in communities', 'Evidence that domesticated_farm animals can be infected and maintain transmissibility of the disease', 'Effectiveness of school distancing', 'Effectiveness of workplace distancing to prevent secondary transmission', 'Effectiveness of community contact reduction', 'Effectiveness of case isolation_isolation of exposed individuals to prevent secondary transmission', 'Effectiveness of personal protective equipment (PPE)', 'Effectiveness of a multifactorial strategy to prevent secondary transmission', 'Seasonality of transmission ', 'How does temperature and humidity affect the transmission of 2019-nCoV', 'What is the likelihood of significant changes in transmissibility in changing seasons']\n",
      "\n",
      "[['effectiveness', 'interinner', 'travel', 'restriction'], ['method', 'understand', 'regulate', 'spread', 'community'], ['evidence', 'domesticatedfarm', 'animal', 'infect', 'maintain', 'transmissibility', 'disease'], ['effectiveness', 'school', 'distance'], ['effectiveness', 'workplace', 'distance', 'prevent', 'secondary', 'transmission'], ['effectiveness', 'community', 'contact', 'reduction'], ['effectiveness', 'case', 'isolationisolation', 'expose', 'individual', 'prevent', 'secondary', 'transmission'], ['effectiveness', 'personal', 'protective', 'equipment', 'ppe'], ['effectiveness', 'multifactorial', 'strategy', 'prevent', 'secondary', 'transmission'], ['seasonality', 'transmission'], ['temperature', 'humidity', 'affect', 'transmission', 'ncov'], ['likelihood', 'change', 'transmissibility', 'change', 'season']]\n"
     ]
    }
   ],
   "source": [
    "questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select(col('finished_clean_lemma').alias('clean_question'))\n",
    "\n",
    "questions = [q.full_text for q in questions.collect()]\n",
    "questions_clean = [q.clean_question for q in questions_clean.collect()]\n",
    "print(questions)\n",
    "print()\n",
    "print(questions_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers: 1329677 rows.\n",
      "After removing empty papers: 406784 rows.\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|\n",
      "|dc079a2e9cf98fad0...|Zoonotic disease ...|\"Veterinary profe...|       1756|[veterinary, prof...|\n",
      "|75af9aa0e63889abd...|Current and Novel...|\"Influenza viruse...|        919|[influenza, virus...|\n",
      "|1755c4785f87bca19...|MERS: Progress on...|Since its identif...|       3942|[identification, ...|\n",
      "|cc829c0f2ab2e110b...|Hepatologie Akute...|\"Das akute Leberv...|       1832|[akute, lebervers...|\n",
      "|ece3d68d9b996c917...|Novel approach to...|\"Introduction | T...|        448|[introduction, qu...|\n",
      "|9cd0f74020b0db181...|On the electrific...|Scientists, polic...|        880|[scientist, polic...|\n",
      "|0b70c1fd82bd1962a...|A dynamic model f...|Infectious diseas...|       7535|[infectious, dise...|\n",
      "|94e8acc14db64cbb1...|Critical evaluati...|Respiratory multi...|       7187|[respiratory, mul...|\n",
      "|d4b11ed79efbb3cd5...|The influence of ...|Airborne transmis...|       5729|[airborne, transm...|\n",
      "|68a2a48d4c67318b0...|Association betwe...|events cannot be ...|        883|[event, observe, ...|\n",
      "|0ccdc351858fd7dfe...|Structural insigh...|Coronavirus is a ...|       3517|[coronavirus, pos...|\n",
      "|81059d5922e947ca8...|Redistributing wo...|\"institutions dea...|        242|[institution, dea...|\n",
      "|d23c6a066a58dbe5e...|LY6E impairs coro...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|\n",
      "|42a6fd4f30e6c37c8...|M2-Polarized Macr...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|\n",
      "|63c9d5537c05b45dd...|Open Access SHORT...|In March and Apri...|       1801|[march, april, sw...|\n",
      "|618cd102ec5051a05...|Mating strategy i...|across species, w...|        917|[species, number,...|\n",
      "|4ae8c9c942c792ce0...|What are the main...|Healthcare-associ...|       1737|[healthcareassoci...|\n",
      "|5af166022c0575cef...|Going to Bat(s) f...|\"An estimated ∼60...|       1673|[estimate, emerge...|\n",
      "|c76e4f5711ab12d65...|Detection and cha...|family, was first...|       2773|[family, detect, ...|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "# Keep only papers with a text length of greater than 10\n",
    "print(\"Before removing empty papers: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "# df = df.dropduplicates(subset='title')\n",
    "# print(\"Removed duplicates\")\n",
    "# df = df.filter(df['text_length'] > 10)\n",
    "print(\"After removing empty papers: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+-----------+--------------------+------------+\n",
      "|            paper_id|               title|           full_text|text_length|        preprocessed|       token|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|   infection|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|       newly|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|      emerge|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|      severe|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|       acute|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...| respiratory|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|    syndrome|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...| coronavirus|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|     sarscov|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|characterize|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|       acute|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|     flulike|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|     symptom|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|    progress|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|       acute|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|        lung|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|      injury|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|       acute|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...| respiratory|\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|    distress|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Explode text\n",
    "exploded = df.withColumn('token', explode(col('preprocessed')))\n",
    "exploded.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get term frequencies\n",
    "tf = exploded \\\n",
    "        .groupBy('paper_id', 'token') \\\n",
    "        .sum() \\\n",
    "        .alias('tf')\n",
    "tf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get inverse document frequencies\n",
    "idf = exploded \\\n",
    "        .groupBy('token') \\\n",
    "        .agg(col('paper_id')) \\\n",
    "        .countDistinct(col('paper_id')) \\\n",
    "        .alias('df')\n",
    "idf.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity (copied, taking inspiration from it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity ----------------------------------------------------------------------\n",
    "def calc_simlarity_score(question_list, text_list,threshold=None, top=None):\n",
    "    if (threshold==None)  and  (top==None):\n",
    "        raise ValueError(\"Parameter `threshold` and `top` cannot both be None\")\n",
    "    dic = {}\n",
    "    tfidf = TfidfVectorizer()\n",
    "    corpus_tfidf_matrix = tfidf.fit_transform(text_list)\n",
    "    ques_tfidf_matrix = tfidf.transform(question_list)\n",
    "    sim_matrix = cosine_similarity(corpus_tfidf_matrix, ques_tfidf_matrix)\n",
    "    for ques_idx in range(sim_matrix.shape[1]):\n",
    "        dic[ques_idx] = []\n",
    "        if threshold != None:\n",
    "            if (threshold>1) or (threshold <0):\n",
    "                raise ValueError(\"Please enter a value from 0 to 1 for parameter `threshold`\")\n",
    "            for paper_idx in range(sim_matrix.shape[0]):\n",
    "                score = sim_matrix[paper_idx, ques_idx]\n",
    "                if score >= threshold:\n",
    "                    dic[ques_idx].append((paper_idx, score))\n",
    "            dic[ques_idx]=sorted(dic[ques_idx], key=lambda i: i[1], reverse=True)\n",
    "        elif top != None:\n",
    "            top_paper_idx_list = sorted(range(len(sim_matrix[:, ques_idx])), key=lambda i: sim_matrix[:,0][i], reverse=True)[:top]\n",
    "            dic[ques_idx] = [(top_idx, sim_matrix[top_idx, ques_idx]) for top_idx in top_paper_idx_list]\n",
    "    return dic, sim_matrix\n",
    "\n",
    "# Retrieve relevant paper----------------------------------------------------------------------\n",
    "def retrieve_paper(df, dic):\n",
    "    df_dic={}\n",
    "    for ques_idx in dic:\n",
    "        new_df = df.iloc[[item[0] for item in dic[ques_idx]], :]\n",
    "        new_df['score'] = [item[1] for item in dic[ques_idx]]\n",
    "        new_df['question'] = questions[ques_idx]\n",
    "        df_dic[ques_idx]=new_df.copy()\n",
    "    return df_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
