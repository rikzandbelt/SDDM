{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for training information retrieval models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, explode, col, countDistinct\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '64g') \\\n",
    "        .config('spark.executor.memory', '32g') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "# .config('spark.memory.fraction', '0.8') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|               title|        list_authors|           full_text|            sections|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|['Xue Wu Zhang', ...|\"The infection of...| 30 drugs were se...|\n",
      "|dc079a2e9cf98fad0...|Zoonotic disease ...|['Charlotte Robin...|\"Veterinary profe...| based on the par...|\n",
      "|75af9aa0e63889abd...|Current and Novel...|['Erasmus Kotey',...|\"Influenza viruse...| although LAIVs a...|\n",
      "|1755c4785f87bca19...|MERS: Progress on...|['*', 'Ryan Aguan...|Since its identif...|['Since its ident...|\n",
      "|cc829c0f2ab2e110b...|Hepatologie Akute...|['Karoline Rutter...|\"Das akute Leberv...| nach Ausschluss ...|\n",
      "|ece3d68d9b996c917...|Novel approach to...|['Ivan Timokhin',...|\"Introduction | T...|      diameter 12 mm|\n",
      "|9cd0f74020b0db181...|On the electrific...|['Martin Weiss', ...|Scientists, polic...|        ''pedelecs\"\"|\n",
      "|0b70c1fd82bd1962a...|A dynamic model f...|['P Raja', 'Sekha...|Infectious diseas...|\"['Infectious dis...|\n",
      "|94e8acc14db64cbb1...|Critical evaluati...|['John D Diaz-Dec...|Respiratory multi...|\"[\"\"Respiratory m...|\n",
      "|d4b11ed79efbb3cd5...|The influence of ...|['Jian Hang', 'Yu...|Airborne transmis...|\"['Airborne trans...|\n",
      "|68a2a48d4c67318b0...|Association betwe...|                  []|events cannot be ...|['events cannot b...|\n",
      "|0ccdc351858fd7dfe...|Structural insigh...|['Manish Sarkar',...|Coronavirus is a ...|\"['Coronavirus is...|\n",
      "|81059d5922e947ca8...|Redistributing wo...|['Marko Ćurković'...|\"institutions dea...|       exceptionally|\n",
      "|d23c6a066a58dbe5e...|LY6E impairs coro...|['Stephanie Pfaen...|\"4 hepatoma cells...| 16 . Pharmacolog...|\n",
      "|42a6fd4f30e6c37c8...|M2-Polarized Macr...|['Tania Cristina'...|Lacaziosis is a c...|\"['Lacaziosis is ...|\n",
      "|63c9d5537c05b45dd...|Open Access SHORT...|['Xiao-Ping Kang'...|In March and Apri...|\"['In March and A...|\n",
      "|618cd102ec5051a05...|Mating strategy i...|['Federica Rosset...|across species, w...|  \"[\"\"across species|\n",
      "|4ae8c9c942c792ce0...|What are the main...|['M C Padoveze', ...|Healthcare-associ...|\"[\"\"Healthcare-as...|\n",
      "|5af166022c0575cef...|Going to Bat(s) f...|['Irah L King', '...|\"An estimated ∼60...| 44) . Although t...|\n",
      "|c76e4f5711ab12d65...|Detection and cha...|['Toshihiro Ito',...|family, was first...|\"['family, was fi...|\n",
      "+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "      .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', col('finished_clean_lemma').alias('clean_question'))\n",
    "\n",
    "# questions = [q.full_text for q in questions.collect()]\n",
    "# questions_clean = [q.clean_question for q in questions_clean.collect()]\n",
    "# print(questions)\n",
    "# print()\n",
    "# print(questions_clean)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers: 1329677 rows.\n",
      "After removing empty papers: 406784 rows.\n",
      "\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|\"The infection of...|        723|[infection, newly...|\n",
      "|dc079a2e9cf98fad0...|Zoonotic disease ...|\"Veterinary profe...|       1756|[veterinary, prof...|\n",
      "|75af9aa0e63889abd...|Current and Novel...|\"Influenza viruse...|        919|[influenza, virus...|\n",
      "|1755c4785f87bca19...|MERS: Progress on...|Since its identif...|       3942|[identification, ...|\n",
      "|cc829c0f2ab2e110b...|Hepatologie Akute...|\"Das akute Leberv...|       1832|[akute, lebervers...|\n",
      "|ece3d68d9b996c917...|Novel approach to...|\"Introduction | T...|        448|[introduction, qu...|\n",
      "|9cd0f74020b0db181...|On the electrific...|Scientists, polic...|        880|[scientist, polic...|\n",
      "|0b70c1fd82bd1962a...|A dynamic model f...|Infectious diseas...|       7535|[infectious, dise...|\n",
      "|94e8acc14db64cbb1...|Critical evaluati...|Respiratory multi...|       7187|[respiratory, mul...|\n",
      "|d4b11ed79efbb3cd5...|The influence of ...|Airborne transmis...|       5729|[airborne, transm...|\n",
      "|68a2a48d4c67318b0...|Association betwe...|events cannot be ...|        883|[event, observe, ...|\n",
      "|0ccdc351858fd7dfe...|Structural insigh...|Coronavirus is a ...|       3517|[coronavirus, pos...|\n",
      "|81059d5922e947ca8...|Redistributing wo...|\"institutions dea...|        242|[institution, dea...|\n",
      "|d23c6a066a58dbe5e...|LY6E impairs coro...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|\n",
      "|42a6fd4f30e6c37c8...|M2-Polarized Macr...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|\n",
      "|63c9d5537c05b45dd...|Open Access SHORT...|In March and Apri...|       1801|[march, april, sw...|\n",
      "|618cd102ec5051a05...|Mating strategy i...|across species, w...|        917|[species, number,...|\n",
      "|4ae8c9c942c792ce0...|What are the main...|Healthcare-associ...|       1737|[healthcareassoci...|\n",
      "|5af166022c0575cef...|Going to Bat(s) f...|\"An estimated ∼60...|       1673|[estimate, emerge...|\n",
      "|c76e4f5711ab12d65...|Detection and cha...|family, was first...|       2773|[family, detect, ...|\n",
      "+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "# Keep only papers with a text length of greater than 10\n",
    "print(\"Before removing empty papers: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "# df = df.dropduplicates(subset='title')\n",
    "# print(\"Removed duplicates\")\n",
    "# df = df.filter(df['text_length'] > 10)\n",
    "print(\"After removing empty papers: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Explode text\n",
    "# tf_idf = df.withColumn('token', explode(col('preprocessed')))\n",
    "# tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get raw term frequencies\n",
    "# tf_idf = tf_idf \\\n",
    "#             .groupBy('paper_id', 'token') \\\n",
    "#             .count()\n",
    "# tf_idf = tf_idf.select('paper_id', 'token', col('count').alias('tf_raw'))\n",
    "# tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Take the log to scale better\n",
    "# def tf(f):\n",
    "#     return 1 + math.log(f)\n",
    "\n",
    "# tf_udf = udf(tf)\n",
    "# tf_idf = tf_idf.select('paper_id', 'token', 'tf_raw', tf('tf_raw').alias('tf'))\n",
    "# tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Get document frequencies\n",
    "# tf_idf = tf_idf \\\n",
    "#             .groupBy('token') \\\n",
    "#             .count()\n",
    "# tf_idf = tf_idf.select('paper_id', 'token', 'tf_raw', 'tf', col('count').alias('df'))\n",
    "# tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Calculate inverse document frequency\n",
    "# num_docs = df.count()\n",
    "# def idf(df):\n",
    "#     return math.log(num_docs/df)\n",
    "\n",
    "# idf_udf = udf(idf)\n",
    "# tf_idf = tf_idf.select('paper_id', 'token', 'tf_raw', 'tf', 'df', idf_udf('count(DISTINCT paper_id)').alias('idf'))\n",
    "# tf_idf.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = HashingTF(inputCol='clean_question', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+\n",
      "|            paper_id|             feature|                norm|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|1329bb2f949e74925...|(262144,[2626,462...|(262144,[2626,462...|\n",
      "|dc079a2e9cf98fad0...|(262144,[353,1466...|(262144,[353,1466...|\n",
      "|75af9aa0e63889abd...|(262144,[1466,170...|(262144,[1466,170...|\n",
      "|1755c4785f87bca19...|(262144,[353,1024...|(262144,[353,1024...|\n",
      "|cc829c0f2ab2e110b...|(262144,[879,1006...|(262144,[879,1006...|\n",
      "|ece3d68d9b996c917...|(262144,[1466,613...|(262144,[1466,613...|\n",
      "|9cd0f74020b0db181...|(262144,[632,882,...|(262144,[632,882,...|\n",
      "|0b70c1fd82bd1962a...|(262144,[661,1466...|(262144,[661,1466...|\n",
      "|94e8acc14db64cbb1...|(262144,[162,1466...|(262144,[162,1466...|\n",
      "|d4b11ed79efbb3cd5...|(262144,[170,269,...|(262144,[170,269,...|\n",
      "|68a2a48d4c67318b0...|(262144,[1284,146...|(262144,[1284,146...|\n",
      "|0ccdc351858fd7dfe...|(262144,[619,1156...|(262144,[619,1156...|\n",
      "|81059d5922e947ca8...|(262144,[2089,278...|(262144,[2089,278...|\n",
      "|d23c6a066a58dbe5e...|(262144,[1466,170...|(262144,[1466,170...|\n",
      "|42a6fd4f30e6c37c8...|(262144,[350,1466...|(262144,[350,1466...|\n",
      "|63c9d5537c05b45dd...|(262144,[267,1466...|(262144,[267,1466...|\n",
      "|618cd102ec5051a05...|(262144,[2969,365...|(262144,[2969,365...|\n",
      "|4ae8c9c942c792ce0...|(262144,[1707,252...|(262144,[1707,252...|\n",
      "|5af166022c0575cef...|(262144,[604,619,...|(262144,[604,619,...|\n",
      "|c76e4f5711ab12d65...|(262144,[1466,170...|(262144,[1466,170...|\n",
      "+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tf_idf_papers.select('paper_id', 'feature', 'norm').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SparseVector(262144, {51736: 0.4027, 73185: 0.4073, 75086: 0.4216, 147758: 0.703})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf_idf_questions.select('question_id', 'feature', 'norm').first()[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute similarity matrix\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions.select('question_id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers.select('paper_id', 'norm') \\\n",
    "                .rdd.map(lambda row: IndexedRow(row.paper_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "sim_matrix = matrix_q.multiply(matrix_p.transpose())\n",
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Similarity (copied, taking inspiration from it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similarity ----------------------------------------------------------------------\n",
    "def calc_simlarity_score(question_list, text_list,threshold=None, top=None):\n",
    "    if (threshold==None)  and  (top==None):\n",
    "        raise ValueError(\"Parameter `threshold` and `top` cannot both be None\")\n",
    "    dic = {}\n",
    "    tfidf = TfidfVectorizer()\n",
    "    corpus_tfidf_matrix = tfidf.fit_transform(text_list)\n",
    "    ques_tfidf_matrix = tfidf.transform(question_list)\n",
    "    sim_matrix = cosine_similarity(corpus_tfidf_matrix, ques_tfidf_matrix)\n",
    "    for ques_idx in range(sim_matrix.shape[1]):\n",
    "        dic[ques_idx] = []\n",
    "        if threshold != None:\n",
    "            if (threshold>1) or (threshold <0):\n",
    "                raise ValueError(\"Please enter a value from 0 to 1 for parameter `threshold`\")\n",
    "            for paper_idx in range(sim_matrix.shape[0]):\n",
    "                score = sim_matrix[paper_idx, ques_idx]\n",
    "                if score >= threshold:\n",
    "                    dic[ques_idx].append((paper_idx, score))\n",
    "            dic[ques_idx]=sorted(dic[ques_idx], key=lambda i: i[1], reverse=True)\n",
    "        elif top != None:\n",
    "            top_paper_idx_list = sorted(range(len(sim_matrix[:, ques_idx])), key=lambda i: sim_matrix[:,0][i], reverse=True)[:top]\n",
    "            dic[ques_idx] = [(top_idx, sim_matrix[top_idx, ques_idx]) for top_idx in top_paper_idx_list]\n",
    "    return dic, sim_matrix\n",
    "\n",
    "# Retrieve relevant paper----------------------------------------------------------------------\n",
    "def retrieve_paper(df, dic):\n",
    "    df_dic={}\n",
    "    for ques_idx in dic:\n",
    "        new_df = df.iloc[[item[0] for item in dic[ques_idx]], :]\n",
    "        new_df['score'] = [item[1] for item in dic[ques_idx]]\n",
    "        new_df['question'] = questions[ques_idx]\n",
    "        df_dic[ques_idx]=new_df.copy()\n",
    "    return df_dic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
