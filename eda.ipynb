{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook performs an Exploratory Data Analysis on the data from the papers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "import spacy\n",
    "import matplotlib as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### First we create a Spark Context  and a SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Just created a SparkContext\n",
      "Just created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# checking if a previous Spark session is active\n",
    "try:\n",
    "    sc = SparkContext(appName=\"SDDM\", master='local[*]')\n",
    "    print(\"Created a SparkContext\")\n",
    "    sqlContext = SQLContext(sc)\n",
    "    print(\"Created a SQLContext\")\n",
    "except ValueError:\n",
    "    print(\"SparkContext already exists in this scope\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Then we load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|            abstract|           full_text|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|d23c6a066a58dbe5e...|LY6E impairs coro...|['Stephanie Pfaen...|Zoonotic coronavi...|\"4 hepatoma cells...|\n",
      "|  1|618cd102ec5051a05...|Mating strategy i...|['Federica Rosset...|Adenoviruses are ...|across species, w...|\n",
      "|  2|ac81102667b0d56ed...|Determination of ...|['V&apos;kovski P...|Positive-sense RN...|\"8 peptide of onl...|\n",
      "|  3|35ac99bfeb665ce0a...|Î¦X174 Attenuation...|['James T Van Leu...|Natural selection...|\"The unequal use ...|\n",
      "|  4|ad146e228bda4e5a3...|Modelling the epi...|['Marco Claudio T...|                null|Late December 201...|\n",
      "|  5|d418cdee18b07a0ad...|Title: Metagenomi...|['Langelier', '; ...|Commentary: Lower...|\"Lower respirator...|\n",
      "|  6|1aa3e788fc6b03c14...|Dark Proteome of ...|['Rajanish Giri',...|Recently emerged ...|\"World health org...|\n",
      "|  7|97e0efc17b5a10c75...|Immune phenotypin...|['Bicheng Zhang',...|                null|Since December 20...|\n",
      "|  8|c954675ee859e2f7b...|Evaluation of the...|['Hui Xu', 'Sufan...|The recent outbre...|\"The recent outbr...|\n",
      "|  9|9896bc65559e6406d...|From Isolation to...|['Yunkai Zhai', '...|                null|The rapid spread ...|\n",
      "| 10|c8559b6ddd6213bc9...|Improving early e...|['Julien Riou', '...|Model-based epide...|\"accounting for s...|\n",
      "| 11|73d80c8f5780d70bd...|Ca 2+ ions promot...|['Marco R Straus'...|Middle East respi...|Coronaviruses (Co...|\n",
      "| 12|b5b029e65b963ae09...|Microbiota-indepe...|['Smita Gopinath'...|Antibiotics are w...|Antibiotics are w...|\n",
      "| 13|6c91b00faa1614242...|Specific ACE2 Exp...|['Xiaoqiang Chai'...|A newly identifie...|In December 2019,...|\n",
      "| 14|fcb76f0907f67a850...|Identification of...|['Rashmi P Mohant...|In solid tumors, ...|\"significantly hi...|\n",
      "| 15|ffbd7555a33770623...|Title: Estimation...|['Ashleigh R Tuit...|                null|CC-BY-NC-ND 4.0 I...|\n",
      "| 16|7852aafdfb9e59e6a...|Detectable serum ...|['Chen Xiaohua', ...|Although the SARS...|The novel coronav...|\n",
      "| 17|7677310b4e43cfaff...|The effects of bo...|['M Pear Hossain'...|The rapid expansi...|\"Meta-population ...|\n",
      "| 18|26cb6703ca72bf978...|Hotspots of aberr...|['Timothy A Dinh'...|Fibrolamellar car...|\"Fibrolamellar ca...|\n",
      "| 19|4b5ba0d8c476c899a...|                null|                  []|                null|(nanobio interfac...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/data.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to close the Spark Context when you are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "[E088] Text of length 1153369 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-88a3cec9061c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mbag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnlp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"done\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'bag_of_words'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'full_text'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbag_of_words\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/pandas/core/series.py\u001b[0m in \u001b[0;36mapply\u001b[0;34m(self, func, convert_dtype, args, **kwds)\u001b[0m\n\u001b[1;32m   3846\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3847\u001b[0m                 \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mastype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobject\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3848\u001b[0;31m                 \u001b[0mmapped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmap_infer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconvert_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3850\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmapped\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSeries\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/lib.pyx\u001b[0m in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/DLNN/lib/python3.7/site-packages/spacy/language.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, text, disable, component_cfg)\u001b[0m\n\u001b[1;32m    427\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m             raise ValueError(\n\u001b[0;32m--> 429\u001b[0;31m                 \u001b[0mErrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mE088\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlength\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_length\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m             )\n\u001b[1;32m    431\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_doc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: [E088] Text of length 1153369 exceeds maximum of 1000000. The v2.x parser and NER models require roughly 1GB of temporary memory per 100,000 characters in the input. This means long texts may cause memory allocation errors. If you're not using the parser or NER, it's probably safe to increase the `nlp.max_length` limit. The limit is in number of characters, so you can check whether your inputs are too long by checking `len(text)`."
     ]
    }
   ],
   "source": [
    "# nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "# # Returns a list of words in a given document while removing punctuation, whitespaces and stop words\n",
    "# def lemmas(doc):\n",
    "#     return [word.lemma_ for word in doc if word.is_alpha and not (word.is_punct or word.is_space or word.is_stop)]\n",
    "\n",
    "\n",
    "# # Returns the words in a given document in a bag of words notation \n",
    "# def bag_of_words(doc):\n",
    "#     bag = {}\n",
    "#     for word in lemmas(doc):\n",
    "#         if word in bag:\n",
    "#             bag[word] += 1\n",
    "#         else:\n",
    "#             bag[word] = 1\n",
    "#     return bag\n",
    "\n",
    "# df['full_text'] = df['full_text'].apply(nlp)\n",
    "# print(\"done\")\n",
    "# df['bag_of_words'] = df['full_text'].apply(bag_of_words)\n",
    "# df.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
