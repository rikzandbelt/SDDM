{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for preprocessing and EDA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "from pyspark.sql import SparkSession\n",
    "# from pyspark import SparkContext\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import size, explode, col\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Spark Context  and a SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.executor.memory', '16g') \\\n",
    "        .config('spark.executor.cores', '5') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|               title|        list_authors|           full_text|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "|1329bb2f949e74925...|Generation of pre...|['Xue Wu Zhang', ...|\"The infection of...|\n",
      "|dc079a2e9cf98fad0...|Zoonotic disease ...|['Charlotte Robin...|\"Veterinary profe...|\n",
      "|75af9aa0e63889abd...|Current and Novel...|['Erasmus Kotey',...|\"Influenza viruse...|\n",
      "|1755c4785f87bca19...|MERS: Progress on...|['*', 'Ryan Aguan...|Since its identif...|\n",
      "|cc829c0f2ab2e110b...|Hepatologie Akute...|['Karoline Rutter...|\"Das akute Leberv...|\n",
      "|ece3d68d9b996c917...|Novel approach to...|['Ivan Timokhin',...|\"Introduction | T...|\n",
      "|9cd0f74020b0db181...|On the electrific...|['Martin Weiss', ...|Scientists, polic...|\n",
      "|0b70c1fd82bd1962a...|A dynamic model f...|['P Raja', 'Sekha...|Infectious diseas...|\n",
      "|94e8acc14db64cbb1...|Critical evaluati...|['John D Diaz-Dec...|Respiratory multi...|\n",
      "|d4b11ed79efbb3cd5...|The influence of ...|['Jian Hang', 'Yu...|Airborne transmis...|\n",
      "|68a2a48d4c67318b0...|Association betwe...|                  []|events cannot be ...|\n",
      "|0ccdc351858fd7dfe...|Structural insigh...|['Manish Sarkar',...|Coronavirus is a ...|\n",
      "|81059d5922e947ca8...|Redistributing wo...|['Marko Ćurković'...|\"institutions dea...|\n",
      "|d23c6a066a58dbe5e...|LY6E impairs coro...|['Stephanie Pfaen...|\"4 hepatoma cells...|\n",
      "|42a6fd4f30e6c37c8...|M2-Polarized Macr...|['Tania Cristina'...|Lacaziosis is a c...|\n",
      "|63c9d5537c05b45dd...|Open Access SHORT...|['Xiao-Ping Kang'...|In March and Apri...|\n",
      "|618cd102ec5051a05...|Mating strategy i...|['Federica Rosset...|across species, w...|\n",
      "|4ae8c9c942c792ce0...|What are the main...|['M C Padoveze', ...|Healthcare-associ...|\n",
      "|5af166022c0575cef...|Going to Bat(s) f...|['Irah L King', '...|\"An estimated ∼60...|\n",
      "|c76e4f5711ab12d65...|Detection and cha...|['Toshihiro Ito',...|family, was first...|\n",
      "+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = (sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/data.csv'))\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words\n",
    "eng_stopwords = stopwords.words('english')\n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(eng_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipelines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess and clean text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers: 1329677 rows.\n",
      "After removing empty papers: 404359 rows.\n",
      "\n",
      "+--------------------+--------------------+-----------+--------------------+\n",
      "|            paper_id|           full_text|text_length|        preprocessed|\n",
      "+--------------------+--------------------+-----------+--------------------+\n",
      "|1329bb2f949e74925...|\"The infection of...|        723|[infection, newly...|\n",
      "|dc079a2e9cf98fad0...|\"Veterinary profe...|       1756|[veterinary, prof...|\n",
      "|75af9aa0e63889abd...|\"Influenza viruse...|        919|[influenza, virus...|\n",
      "|1755c4785f87bca19...|Since its identif...|       3942|[since, identific...|\n",
      "|cc829c0f2ab2e110b...|\"Das akute Leberv...|       1832|[das, akute, lebe...|\n",
      "|ece3d68d9b996c917...|\"Introduction | T...|        448|[introduction, qu...|\n",
      "|9cd0f74020b0db181...|Scientists, polic...|        880|[scientist, polic...|\n",
      "|0b70c1fd82bd1962a...|Infectious diseas...|       7535|[infectious, dise...|\n",
      "|94e8acc14db64cbb1...|Respiratory multi...|       7187|[respiratory, mul...|\n",
      "|d4b11ed79efbb3cd5...|Airborne transmis...|       5729|[airborne, transm...|\n",
      "|68a2a48d4c67318b0...|events cannot be ...|        883|[event, cannot, d...|\n",
      "|0ccdc351858fd7dfe...|Coronavirus is a ...|       3517|[coronavirus, pos...|\n",
      "|81059d5922e947ca8...|\"institutions dea...|        242|[institution, dea...|\n",
      "|d23c6a066a58dbe5e...|\"4 hepatoma cells...|        647|[hepatoma, cell, ...|\n",
      "|42a6fd4f30e6c37c8...|Lacaziosis is a c...|       2288|[lacaziosis, chro...|\n",
      "|63c9d5537c05b45dd...|In March and Apri...|       1801|[march, april, no...|\n",
      "|618cd102ec5051a05...|across species, w...|        917|[across, species,...|\n",
      "|4ae8c9c942c792ce0...|Healthcare-associ...|       1737|[healthcareassoci...|\n",
      "|5af166022c0575cef...|\"An estimated ∼60...|       1673|[estimate, emerge...|\n",
      "|c76e4f5711ab12d65...|family, was first...|       2773|[family, first, d...|\n",
      "+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "# Keep only papers with a text length of greater than 10\n",
    "print(\"Before removing empty papers: {} rows.\".format(df.count()))\n",
    "df = df.dropna(subset='full_text')\n",
    "# df = df.dropduplicates(subset='full_text')\n",
    "# print(\"Removed duplicates\")\n",
    "# df = df.filter(df['text_length'] > 10)\n",
    "print(\"After removing empty papers: {} rows.\".format(df.count()))\n",
    "print()\n",
    "\n",
    "df_text = df.select(\n",
    "            'paper_id',\n",
    "            'full_text',\n",
    "            'text_length',\n",
    "            col('finished_clean_lemma').alias('preprocessed'))\n",
    "\n",
    "df = df.select(\n",
    "            'paper_id',\n",
    "            'title',\n",
    "            'list_authors',\n",
    "            'full_text')\n",
    "\n",
    "df_text.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analyze text lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+\n",
      "|summary|      text_length|\n",
      "+-------+-----------------+\n",
      "|  count|           404359|\n",
      "|   mean|633.9327429338781|\n",
      "| stddev|2344.178275750687|\n",
      "|    min|                0|\n",
      "|    max|           253534|\n",
      "+-------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df_text.select('text_length').describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count Vectorize text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_text_expl = df_text.withColumn('exploded_text', explode(col('preprocessed')))\n",
    "\n",
    "counts = df_text_expl.groupby('exploded_text').count()\n",
    "counts_df = counts.toPandas()\n",
    "counts_dict = {counts_df.loc[i, 'exploded_text']:counts_df.loc[i, 'count'] for i in range(counts_df.shape[0])}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from wordcloud import WordCloud\n",
    "\n",
    "# df_pd = df_text.toPandas() #converting spark dataframes to pandas dataframes\n",
    "# df_pd.plot.barh(x='Keywords', y='Frequency', rot=1, figsize=(10,8))\n",
    "\n",
    "# wordcloudConvertDF = df_pd.set_index('Keywords').T.to_dict('records')\n",
    "wordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=100, relative_scaling=0.5) \\\n",
    "                .generate_from_frequencies(counts_df)\n",
    "plt.figure(figsize=(14, 10))    \n",
    "plt.imshow(wordcloud, interpolation=\"bilinear\")\n",
    "plt.axis('off')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Don't forget to close the Spark Context when you are done!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
