{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, col, countDistinct, collect_list, monotonically_increasing_id, row_number\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import os\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stop words\n",
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right paths on local machine\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.7'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '8g') \\\n",
    "        .config('spark.executor.memory', '8g') \\\n",
    "        .config('spark.memory.fraction', '0.8') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.local.dir', '/home/rikz/Documents/Master/Semester2/SDDM/data/tmp') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "#         .config('spark.local.dir', '/data/s1847503/SDDM/tmp') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|           question0|                   -|                   -|How does temperat...|\n",
      "|  1|           question1|                   -|                   -|Seasonality of tr...|\n",
      "|  2|           question2|                   -|                   -|Effectiveness of ...|\n",
      "|  3|           question3|                   -|                   -|Effectiveness of ...|\n",
      "|  4|           question4|                   -|                   -|Effectiveness of ...|\n",
      "|  5|           question5|                   -|                   -|Effectiveness of ...|\n",
      "|  6|           question6|                   -|                   -|Effectiveness of ...|\n",
      "|  7|           question7|                   -|                   -|Effectiveness of ...|\n",
      "|  8|           question8|                   -|                   -|Significant chang...|\n",
      "|  9|           question9|                   -|                   -|Effectiveness of ...|\n",
      "| 10|20ee844014e6b7c91...|Coronavirus disea...|['John P A Ioanni...|\"The evolving cor...|\n",
      "| 11|3d2e51a4d7e9699e4...|COVID-19 and mate...|                  []|tems to ensure fo...|\n",
      "| 12|dd4cba9cda9f49ba9...|Systematic review...|                  []|Coronavirus has k...|\n",
      "| 13|2ef0ac31fd85f28b5...|SARS-CoV-Encoded ...|['Luc√≠a Morales',...|In Brief SARS-CoV...|\n",
      "| 14|4bae3f6031a50a23b...|Effect of HA330 r...|['Xuefeng Xu', 'C...|\"The acute respir...|\n",
      "| 15|150b0fed0020d4549...|Traditional Chine...|           ['Ke He']|\"Traditional medi...|\n",
      "| 16|0ed26fea4f7e1a99d...|A new coronavirus...|['Fan Wu', 'Su Zh...|a Amino acids of ...|\n",
      "| 17|4b9e5d0ffdac80fba...|Article history: ...|['Kyung Sook Jung...|The major communi...|\n",
      "| 18|41fadbfc4def2200b...|49 Intellectual P...|                  []|\"Use of biotechno...|\n",
      "| 19|a8a2882316256ca57...|Region-resolved p...|['Hao-Liang Hu', ...|especially mitral...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "        .load('/home/rikz/Documents/Master/Semester2/SDDM/data/data.csv')\n",
    "#       .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|publish_time|               title|                 doi|             journal|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|d1aafb70c066a2068...|  2001-07-04|Clinical features...|10.1186/1471-2334...|      BMC Infect Dis|\n",
      "|6b0567729c2143a66...|  2000-08-15|Nitric oxide: a p...|        10.1186/rr14|          Respir Res|\n",
      "|06ced00a5fc042159...|  2000-08-25|Surfactant protei...|        10.1186/rr19|          Respir Res|\n",
      "|348055649b6b8cf2b...|  2001-02-22|Role of endotheli...|        10.1186/rr44|          Respir Res|\n",
      "|5f48792a5fa08bed9...|  2001-05-11|Gene expression i...|        10.1186/rr61|          Respir Res|\n",
      "|b2897e1277f566411...|  2001-12-17|Sequence requirem...|10.1093/emboj/20....|    The EMBO Journal|\n",
      "|3bb07ea10432f7738...|  2001-03-08|Debate: Transfusi...|       10.1186/cc987|           Crit Care|\n",
      "|5806726a24dc91de3...|  2001-05-02|The 21st Internat...|      10.1186/cc1013|           Crit Care|\n",
      "|faaf1022ccfe93b03...|  2003-08-07|Heme oxygenase-1 ...|10.1186/1465-9921...|          Respir Res|\n",
      "|5b44feca5d6ffaaeb...|  2003-09-01|Technical Descrip...| 10.1197/jamia.m1345|Journal of the Am...|\n",
      "|9d4e3e8eb092d5ed2...|  2000-04-17|Conservation of p...|10.1093/emboj/19....|              EMBO J|\n",
      "|14e0cac6e86d62859...|  2000-09-01|Heterogeneous nuc...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|d09b79026117ec9fa...|  2003-12-12|A Method to Ident...|       10.1251/bpo66|  Biol Proced Online|\n",
      "|44102e3e69e70ad2a...|  2000-08-01|Vaccinia virus in...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|6e8517cb25ff228cb...|  2004-01-20|The site of origi...|10.1186/1479-5876...|        J Transl Med|\n",
      "|30a4842a2e257f725...|  2004-05-26|Multi-faceted, mu...|10.1186/1742-4690...|       Retrovirology|\n",
      "|6a8ac55ea2a1fbd99...|  2004-03-31|Herpes simplex vi...|      10.1186/cc2850|           Crit Care|\n",
      "|367af6bb9a8bbda02...|  2004-08-06|Logistics of comm...|10.1186/1471-2458...|   BMC Public Health|\n",
      "|4df2c6eecb985fcb2...|  2004-09-27|Protection of pul...|10.1186/1465-9921...|          Respir Res|\n",
      "|83b05e8afa6cbe7a6...|  2005-01-03|Bioinformatic map...|10.1186/1471-2164...|        BMC Genomics|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "df_metadata = sqlContext.read.format('csv').options(header='true') \\\n",
    "                .load('/home/rikz/Documents/Master/Semester2/SDDM/data/metadata.csv') \\\n",
    "                .select(col('sha').alias('paper_id'), 'publish_time', 'title', 'doi', 'journal')\n",
    "\n",
    "df_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|question_id|           full_text|        preprocessed|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          0|How does temperat...|[temperature, hum...|\n",
      "|          1|Seasonality of tr...|[seasonality, tra...|\n",
      "|          2|Effectiveness of ...|[effectiveness, i...|\n",
      "|          3|Effectiveness of ...|[effectiveness, p...|\n",
      "|          4|Effectiveness of ...|[effectiveness, s...|\n",
      "|          5|Effectiveness of ...|[effectiveness, c...|\n",
      "|          6|Effectiveness of ...|[effectiveness, m...|\n",
      "|          7|Effectiveness of ...|[effectiveness, c...|\n",
      "|          8|Significant chang...|[change, transmis...|\n",
      "|          9|Effectiveness of ...|[effectiveness, w...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions = sqlContext.read.format('csv').options(header='true').load('/home/rikz/Documents/Master/Semester2/SDDM/data/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', 'full_text', col('finished_clean_lemma').alias('preprocessed'))\n",
    "questions_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Effectiveness of inter inner travel restriction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a question (from 0 to 9)\n",
    "question_num = 2\n",
    "\n",
    "questions_clean = questions_clean.filter(questions_clean.question_id == question_num)\n",
    "q = questions_clean.first().full_text\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Removed empty papers\n",
      "Removed duplicates\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| id|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "|263|6cb2eced687ea9da4...|Medical recommend...|\"In early 2020, t...|       1224|[early, face, glo...|\n",
      "|468|9482e5881613ae262...|Oral vaccination ...|Helicobacter pylo...|       3705|[helicobacter, py...|\n",
      "|491|c3ba4e042c5173d4a...|Thomas Gr√ºnewald,...|\"Die Versorgung v...|        205|[versorgung, pati...|\n",
      "|173|e639fc8b330785fb2...|            Vaccines|\"Since vaccinatio...|        537|[vaccination, doc...|\n",
      "|201|8645437ad8a6f8538...|Accessibility and...|The terminology o...|       8079|[terminology, acc...|\n",
      "|224|121638b718d18f7bb...|JOURNAL OF MEDICA...|\"Adenoviruses are...|        793|[adenoviruses, do...|\n",
      "|200|4bc77a5504262d2f8...|A Fuzzy Model for...|\"These rates also...|         67|[rate, impact, pr...|\n",
      "|144|c4213fb7b0fdd8926...|Forecasting COVID...|ORCID iD: https:/...|        990|[orcid, httpsorci...|\n",
      "|249|cc367798b29defe46...|Modeling State In...|\"A and B are fixe...|        219|[matrix, vector, ...|\n",
      "|131|0823046d9ca5204f9...|Is the anti-psych...|\"Severe acute res...|       2103|[severe, acute, r...|\n",
      "| 62|249562b091482dd3e...|Viral Respiratory...|Common viral resp...|       1795|[common, viral, r...|\n",
      "|245|1043007c52a94b83c...|A Major Role of M...|MHV3 constitutes ...|       1933|[mhv, constitute,...|\n",
      "|183|71d2ad06792c5adb2...|Immune-complex gl...|Recently, the Wor...|       3657|[small, animal, v...|\n",
      "|305|b8d39ed5f718e5510...|Idiopathic Inters...|Over 200 causes o...|       2686|[interstitial, lu...|\n",
      "|501|f9426eca19bf968f0...|\"\"\"We're staying ...|['Andr√©s Losada-B...|         27|[andr√©s, losadaba...|\n",
      "| 61|714dd1fe1309414e8...|Several countries...|It has been few m...|       4266|[month, confirm, ...|\n",
      "|223|b83d5dc2ac33c3e6d...|Spectrum of Virus...|\"istered, laborat...|        608|[istered, laborat...|\n",
      "|383|cda1f45974f3f33fa...|Sophie Knipper 1 ...|\"In order to rest...|        111|[order, restrain,...|\n",
      "| 84|e2e23d6985639b6aa...|Specific Asparagi...|\"added purified p...|       1578|[add, purify, pop...|\n",
      "|172|3a143e7e5417c8df9...|Machine intellige...|\"A cluster of pne...|       2421|[cluster, pneumon...|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "df = df.dropna(subset=['paper_id', 'full_text', 'title'])\n",
    "print(\"Removed empty papers\")\n",
    "df = df.dropDuplicates(subset=['paper_id', 'full_text'])\n",
    "print(\"Removed duplicates\")\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                col('_c0').alias('id'),\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing time: 11.612033367156982 sec\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing time: {} sec'.format(time_after-time_before) )\n",
    "# Small dataset: ~15 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = []\n",
    "tf_idf_papers = []\n",
    "\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = []\n",
    "tf_idf_questions = []\n",
    "\n",
    "tf_q = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any extra columns\n",
    "tf_idf_questions = tf_idf_questions.select('question_id', 'feature')\n",
    "tf_idf_papers = tf_idf_papers.select('id', 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change TF-IDF matrices to block matrices so they can be multiplied\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions \\\n",
    "                    .select('question_id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers \\\n",
    "                    .select('id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the matrices to get the cosine similarity matrix\n",
    "sim_matrix = matrix_p.multiply(matrix_q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the similarity matrix to an array\n",
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|          similarity|\n",
      "+---+--------------------+\n",
      "|304|  0.2511019958412871|\n",
      "|206| 0.08536616840188067|\n",
      "|190| 0.07452061862071889|\n",
      "|163|0.043100103389574054|\n",
      "|201| 0.04124321899075376|\n",
      "|409| 0.03941798954163997|\n",
      "|147|0.036932171719703324|\n",
      "|489|0.035389424661473295|\n",
      "|383| 0.03373650773383759|\n",
      "|223| 0.03326527532254628|\n",
      "+---+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the similarity matrix to a dataframe again to get the most relevant papers for the selected question\n",
    "relevant = sc.parallelize(sim_matrix[:, question_num].tolist()) \\\n",
    "                .zipWithIndex() \\\n",
    "                .toDF(['similarity', 'id'])\n",
    "\n",
    "# Remove questions from the paper list\n",
    "# Sort on cosine similarity\n",
    "# Take the top 10 relevant documents\n",
    "relevant = relevant.select('id', 'similarity') \\\n",
    "                .filter(relevant.id > 9) \\\n",
    "                .sort(col('similarity').desc()) \\\n",
    "                .limit(10)\n",
    "\n",
    "relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Query: Effectiveness of inter inner travel restriction\n",
      "\n",
      "Relevant Papers:\n",
      "\n",
      "+--------------------+--------------------+--------------------+\n",
      "|            paper_id|         paper_title|          similarity|\n",
      "+--------------------+--------------------+--------------------+\n",
      "|2d1300359d8ec1682...|Epidemiological a...|  0.2511019958412871|\n",
      "|4be9882705e5d0021...|Development of th...| 0.08536616840188067|\n",
      "|a8b5de09c002605e9...|BMC Medicine Calc...| 0.07452061862071889|\n",
      "|9735c289f323e4ede...|Geriatric mental ...|0.043100103389574054|\n",
      "|8645437ad8a6f8538...|Accessibility and...| 0.04124321899075376|\n",
      "|a1bbca4d6a52f9747...|Journal Pre-proof...| 0.03941798954163997|\n",
      "|dc7726fbace94aca0...|Analysis of Effec...|0.036932171719703324|\n",
      "|2307ca38237d9b328...|Preparing for unc...|0.035389424661473295|\n",
      "|cda1f45974f3f33fa...|Sophie Knipper 1 ...| 0.03373650773383759|\n",
      "|b83d5dc2ac33c3e6d...|Spectrum of Virus...| 0.03326527532254628|\n",
      "+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Get the data of the 10 most relevant papers in order of relevance\n",
    "relevant_ids = [int(row.id) for row in relevant.collect()]\n",
    "print(\"Query: {}\".format(q))\n",
    "print()\n",
    "print(\"Relevant Papers:\")\n",
    "print()\n",
    "df_relevant = relevant.join(df.filter(df.id.isin(relevant_ids)), on=['id'], how='left_outer') \\\n",
    "                        .select('paper_id', col('title').alias('paper_title'), 'similarity')\n",
    "df_relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>paper_id</th>\n",
       "      <th>publish_time</th>\n",
       "      <th>paper_title</th>\n",
       "      <th>doi</th>\n",
       "      <th>journal</th>\n",
       "      <th>similarity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2d1300359d8ec1682b0b6bdb51df4604a7ed4930</td>\n",
       "      <td>2019-09-27</td>\n",
       "      <td>Epidemiological and clinical profile of Korean...</td>\n",
       "      <td>10.1097/md.0000000000017330</td>\n",
       "      <td>Medicine (Baltimore)</td>\n",
       "      <td>0.251102</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4be9882705e5d0021b60347ea0ac3e2ae6d91b40</td>\n",
       "      <td>2020-02-11</td>\n",
       "      <td>Development of the WHO-INTEGRATE evidence-to-d...</td>\n",
       "      <td>10.1186/s12962-020-0203-6</td>\n",
       "      <td>Cost Eff Resour Alloc</td>\n",
       "      <td>0.085366</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>a8b5de09c002605e9748e653b5ca827a23e83cf3</td>\n",
       "      <td>2009-12-24</td>\n",
       "      <td>BMC Medicine Calculating the potential for wit...</td>\n",
       "      <td>10.1186/1741-7015-7-81</td>\n",
       "      <td>BMC Med</td>\n",
       "      <td>0.074521</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>9735c289f323e4edefb7532889f9fa56d35a52cf</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Geriatric mental health and COVID-19: An eye-o...</td>\n",
       "      <td>10.1016/j.jagp.2020.05.009</td>\n",
       "      <td>Am J Geriatr Psychiatry</td>\n",
       "      <td>0.043100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>8645437ad8a6f8538447b79ff2652174d9aba0d0</td>\n",
       "      <td>2020-05-11</td>\n",
       "      <td>Accessibility and site suitability for healthc...</td>\n",
       "      <td>10.1007/s41324-020-00330-0</td>\n",
       "      <td>Spat</td>\n",
       "      <td>0.041243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>a1bbca4d6a52f9747ea0159d11b5493da1f0b29c</td>\n",
       "      <td>2020-04-10</td>\n",
       "      <td>Journal Pre-proofs Psychiatry hospital managem...</td>\n",
       "      <td>10.1016/j.bbi.2020.04.018</td>\n",
       "      <td>Brain Behav Immun</td>\n",
       "      <td>0.039418</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>dc7726fbace94aca0f29a112cafe36198734b13a</td>\n",
       "      <td>2020-04-26</td>\n",
       "      <td>Analysis of Effectiveness of Quarantine Measur...</td>\n",
       "      <td>10.1101/2020.04.21.20074245</td>\n",
       "      <td>None</td>\n",
       "      <td>0.036932</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2307ca38237d9b328510976a5cf6ac8dbd0a733a</td>\n",
       "      <td>2020-03-31</td>\n",
       "      <td>Preparing for uncertainty during public health...</td>\n",
       "      <td>10.1177/0840470420917172</td>\n",
       "      <td>Healthc Manage Forum</td>\n",
       "      <td>0.035389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>cda1f45974f3f33fa7ebb605b4a6618adf187599</td>\n",
       "      <td>2020-05-18</td>\n",
       "      <td>Sophie Knipper 1 (SK), Franziska von Breunig 2...</td>\n",
       "      <td>10.1111/bju.15115</td>\n",
       "      <td>BJU Int</td>\n",
       "      <td>0.033737</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>b83d5dc2ac33c3e6d7c8660852562e485504aa28</td>\n",
       "      <td>2007-03-01</td>\n",
       "      <td>Spectrum of Viruses and Atypical Bacteria in I...</td>\n",
       "      <td>10.1086/511432</td>\n",
       "      <td>J Infect Dis</td>\n",
       "      <td>0.033265</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                   paper_id publish_time  \\\n",
       "9  2d1300359d8ec1682b0b6bdb51df4604a7ed4930   2019-09-27   \n",
       "2  4be9882705e5d0021b60347ea0ac3e2ae6d91b40   2020-02-11   \n",
       "1  a8b5de09c002605e9748e653b5ca827a23e83cf3   2009-12-24   \n",
       "8  9735c289f323e4edefb7532889f9fa56d35a52cf   2020-05-18   \n",
       "6  8645437ad8a6f8538447b79ff2652174d9aba0d0   2020-05-11   \n",
       "4  a1bbca4d6a52f9747ea0159d11b5493da1f0b29c   2020-04-10   \n",
       "5  dc7726fbace94aca0f29a112cafe36198734b13a   2020-04-26   \n",
       "0  2307ca38237d9b328510976a5cf6ac8dbd0a733a   2020-03-31   \n",
       "7  cda1f45974f3f33fa7ebb605b4a6618adf187599   2020-05-18   \n",
       "3  b83d5dc2ac33c3e6d7c8660852562e485504aa28   2007-03-01   \n",
       "\n",
       "                                         paper_title  \\\n",
       "9  Epidemiological and clinical profile of Korean...   \n",
       "2  Development of the WHO-INTEGRATE evidence-to-d...   \n",
       "1  BMC Medicine Calculating the potential for wit...   \n",
       "8  Geriatric mental health and COVID-19: An eye-o...   \n",
       "6  Accessibility and site suitability for healthc...   \n",
       "4  Journal Pre-proofs Psychiatry hospital managem...   \n",
       "5  Analysis of Effectiveness of Quarantine Measur...   \n",
       "0  Preparing for uncertainty during public health...   \n",
       "7  Sophie Knipper 1 (SK), Franziska von Breunig 2...   \n",
       "3  Spectrum of Viruses and Atypical Bacteria in I...   \n",
       "\n",
       "                           doi                  journal  similarity  \n",
       "9  10.1097/md.0000000000017330     Medicine (Baltimore)    0.251102  \n",
       "2    10.1186/s12962-020-0203-6    Cost Eff Resour Alloc    0.085366  \n",
       "1       10.1186/1741-7015-7-81                  BMC Med    0.074521  \n",
       "8   10.1016/j.jagp.2020.05.009  Am J Geriatr Psychiatry    0.043100  \n",
       "6   10.1007/s41324-020-00330-0                     Spat    0.041243  \n",
       "4    10.1016/j.bbi.2020.04.018        Brain Behav Immun    0.039418  \n",
       "5  10.1101/2020.04.21.20074245                     None    0.036932  \n",
       "0     10.1177/0840470420917172     Healthc Manage Forum    0.035389  \n",
       "7            10.1111/bju.15115                  BJU Int    0.033737  \n",
       "3               10.1086/511432             J Infect Dis    0.033265  "
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the summary table with the relevant paper from the metadata\n",
    "df_relevant = df_relevant.join(df_metadata, on=['paper_id'], how='left_outer') \\\n",
    "                            .select('paper_id', 'publish_time', 'paper_title', 'doi', 'journal', 'similarity') \\\n",
    "                            .toPandas() \\\n",
    "                            .sort_values(by='similarity', ascending=False)\n",
    "df_relevant.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary table extracted and sent to csv file.\n"
     ]
    }
   ],
   "source": [
    "# Send the summary table to a csv file\n",
    "df_relevant.to_csv('/home/rikz/Documents/Master/Semester2/SDDM/SDDM/summary_tables/{}.csv' \\\n",
    "                   .format(q.lower().replace(' ', '_')), index=False)\n",
    "print(\"Summary table extracted and sent to csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
