{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Notebook for the TF-IDF model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark import SQLContext\n",
    "from pyspark.sql.functions import udf, size, col, countDistinct, collect_list, monotonically_increasing_id, row_number\n",
    "from pyspark.ml import Pipeline\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.parsing.preprocessing import STOPWORDS as gensim_words\n",
    "import spacy\n",
    "sp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import os\n",
    "\n",
    "from sparknlp.base import Finisher, DocumentAssembler\n",
    "from sparknlp.annotator import Tokenizer, Normalizer, LemmatizerModel, StopWordsCleaner\n",
    "\n",
    "from pyspark.ml.feature import HashingTF, IDF\n",
    "from pyspark.ml.feature import Normalizer as Normalizer_L2\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import stop words\n",
    "nltk_stopwords = set(stopwords.words('english')) \\\n",
    "                    .union(set(stopwords.words('german'))) \\\n",
    "                    .union(set(stopwords.words('french')))\n",
    "gensim_stopwords = set(gensim_words)\n",
    "spacy_stopwords = sp.Defaults.stop_words\n",
    "# https://countwordsfree.com/stopwords\n",
    "cwf_stopwords = set(line.strip() for line in open('stop_words.txt'))\n",
    "\n",
    "all_stopwords = list( nltk_stopwords \\\n",
    "                        .union(gensim_stopwords) \\\n",
    "                        .union(spacy_stopwords) \\\n",
    "                        .union(cwf_stopwords) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Spark Context and SQL Context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the right paths on local machine\n",
    "os.environ['JAVA_HOME'] = '/usr/lib/jvm/java-8-openjdk-amd64'\n",
    "os.environ[\"PYSPARK_PYTHON\"] = '/usr/bin/python3.7'\n",
    "os.environ[\"PYSPARK_DRIVER_PYTHON\"] = '/usr/bin/python3.7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created a SparkSession\n",
      "Created a SparkContext\n",
      "Created a SQLContext\n"
     ]
    }
   ],
   "source": [
    "# Start spark session configured for spark nlp\n",
    "spark = SparkSession.builder \\\n",
    "        .master('local[*]') \\\n",
    "        .appName('SDDM') \\\n",
    "        .config('spark.driver.memory', '8g') \\\n",
    "        .config('spark.executor.memory', '8g') \\\n",
    "        .config('spark.memory.fraction', '0.8') \\\n",
    "        .config('spark.executor.cores', '8') \\\n",
    "        .config('spark.local.dir', '/home/rikz/Documents/Master/Semester2/SDDM/data/tmp') \\\n",
    "        .config('spark.jars.packages', 'com.johnsnowlabs.nlp:spark-nlp_2.11:2.5.0') \\\n",
    "        .getOrCreate()\n",
    "print(\"Created a SparkSession\")\n",
    "sc = spark.sparkContext\n",
    "print(\"Created a SparkContext\")\n",
    "sqlContext = SQLContext(sc)\n",
    "print(\"Created a SQLContext\")\n",
    "\n",
    "#         .config('spark.local.dir', '/data/s1847503/SDDM/tmp') \\"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the data into a SQLContext Dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|_c0|            paper_id|               title|        list_authors|           full_text|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "|  0|           question0|                   -|                   -|How does temperat...|\n",
      "|  1|           question1|                   -|                   -|Seasonality of tr...|\n",
      "|  2|           question2|                   -|                   -|Effectiveness of ...|\n",
      "|  3|           question3|                   -|                   -|Effectiveness of ...|\n",
      "|  4|           question4|                   -|                   -|Effectiveness of ...|\n",
      "|  5|           question5|                   -|                   -|Effectiveness of ...|\n",
      "|  6|           question6|                   -|                   -|Effectiveness of ...|\n",
      "|  7|           question7|                   -|                   -|Effectiveness of ...|\n",
      "|  8|           question8|                   -|                   -|Significant chang...|\n",
      "|  9|           question9|                   -|                   -|Effectiveness of ...|\n",
      "| 10|20ee844014e6b7c91...|Coronavirus disea...|['John P A Ioanni...|\"The evolving cor...|\n",
      "| 11|3d2e51a4d7e9699e4...|COVID-19 and mate...|                  []|tems to ensure fo...|\n",
      "| 12|dd4cba9cda9f49ba9...|Systematic review...|                  []|Coronavirus has k...|\n",
      "| 13|2ef0ac31fd85f28b5...|SARS-CoV-Encoded ...|['Luc√≠a Morales',...|In Brief SARS-CoV...|\n",
      "| 14|4bae3f6031a50a23b...|Effect of HA330 r...|['Xuefeng Xu', 'C...|\"The acute respir...|\n",
      "| 15|150b0fed0020d4549...|Traditional Chine...|           ['Ke He']|\"Traditional medi...|\n",
      "| 16|0ed26fea4f7e1a99d...|A new coronavirus...|['Fan Wu', 'Su Zh...|a Amino acids of ...|\n",
      "| 17|4b9e5d0ffdac80fba...|Article history: ...|['Kyung Sook Jung...|The major communi...|\n",
      "| 18|41fadbfc4def2200b...|49 Intellectual P...|                  []|\"Use of biotechno...|\n",
      "| 19|a8a2882316256ca57...|Region-resolved p...|['Hao-Liang Hu', ...|especially mitral...|\n",
      "+---+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load data\n",
    "df = sqlContext.read.format('csv').options(header='true', maxColumns=2000000) \\\n",
    "        .load('/home/rikz/Documents/Master/Semester2/SDDM/data/data.csv')\n",
    "#       .load('/data/s1847503/SDDM/newdata/data.csv')\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|            paper_id|publish_time|               title|                 doi|             journal|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "|d1aafb70c066a2068...|  2001-07-04|Clinical features...|10.1186/1471-2334...|      BMC Infect Dis|\n",
      "|6b0567729c2143a66...|  2000-08-15|Nitric oxide: a p...|        10.1186/rr14|          Respir Res|\n",
      "|06ced00a5fc042159...|  2000-08-25|Surfactant protei...|        10.1186/rr19|          Respir Res|\n",
      "|348055649b6b8cf2b...|  2001-02-22|Role of endotheli...|        10.1186/rr44|          Respir Res|\n",
      "|5f48792a5fa08bed9...|  2001-05-11|Gene expression i...|        10.1186/rr61|          Respir Res|\n",
      "|b2897e1277f566411...|  2001-12-17|Sequence requirem...|10.1093/emboj/20....|    The EMBO Journal|\n",
      "|3bb07ea10432f7738...|  2001-03-08|Debate: Transfusi...|       10.1186/cc987|           Crit Care|\n",
      "|5806726a24dc91de3...|  2001-05-02|The 21st Internat...|      10.1186/cc1013|           Crit Care|\n",
      "|faaf1022ccfe93b03...|  2003-08-07|Heme oxygenase-1 ...|10.1186/1465-9921...|          Respir Res|\n",
      "|5b44feca5d6ffaaeb...|  2003-09-01|Technical Descrip...| 10.1197/jamia.m1345|Journal of the Am...|\n",
      "|9d4e3e8eb092d5ed2...|  2000-04-17|Conservation of p...|10.1093/emboj/19....|              EMBO J|\n",
      "|14e0cac6e86d62859...|  2000-09-01|Heterogeneous nuc...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|d09b79026117ec9fa...|  2003-12-12|A Method to Ident...|       10.1251/bpo66|  Biol Proced Online|\n",
      "|44102e3e69e70ad2a...|  2000-08-01|Vaccinia virus in...|10.1093/emboj/19....|    The EMBO Journal|\n",
      "|6e8517cb25ff228cb...|  2004-01-20|The site of origi...|10.1186/1479-5876...|        J Transl Med|\n",
      "|30a4842a2e257f725...|  2004-05-26|Multi-faceted, mu...|10.1186/1742-4690...|       Retrovirology|\n",
      "|6a8ac55ea2a1fbd99...|  2004-03-31|Herpes simplex vi...|      10.1186/cc2850|           Crit Care|\n",
      "|367af6bb9a8bbda02...|  2004-08-06|Logistics of comm...|10.1186/1471-2458...|   BMC Public Health|\n",
      "|4df2c6eecb985fcb2...|  2004-09-27|Protection of pul...|10.1186/1465-9921...|          Respir Res|\n",
      "|83b05e8afa6cbe7a6...|  2005-01-03|Bioinformatic map...|10.1186/1471-2164...|        BMC Genomics|\n",
      "+--------------------+------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Load metadata\n",
    "df_metadata = sqlContext.read.format('csv').options(header='true') \\\n",
    "                .load('/home/rikz/Documents/Master/Semester2/SDDM/data/metadata.csv') \\\n",
    "                .select(col('sha').alias('paper_id'), 'publish_time', 'title', 'doi', 'journal')\n",
    "\n",
    "df_metadata.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize Annotators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lemma_antbnc download started this may take some time.\n",
      "Approximate size to download 907.6 KB\n",
      "[OK!]\n"
     ]
    }
   ],
   "source": [
    "# Pipeline for text\n",
    "document_assembler = DocumentAssembler() \\\n",
    "                        .setInputCol('full_text') \\\n",
    "                        .setOutputCol('document')\n",
    "\n",
    "# Tokenizer divides the text into tokens\n",
    "tokenizer = Tokenizer() \\\n",
    "                .setInputCols(['document']) \\\n",
    "                .setOutputCol('tokens')\n",
    "\n",
    "# Finisher converts tokens to human-readable output (we need the tokens for determining the text lengths)\n",
    "finisher_tokens = Finisher() \\\n",
    "                        .setInputCols(['tokens']) \\\n",
    "                        .setCleanAnnotations(False)\n",
    "\n",
    "# Normalizer removes punctuation, numbers etc.\n",
    "normalizer = Normalizer() \\\n",
    "                .setInputCols(['tokens']) \\\n",
    "                .setOutputCol('normalized') \\\n",
    "                .setLowercase(True)\n",
    "\n",
    "# Lemmatizer changes each word to its lemma\n",
    "lemmatizer = LemmatizerModel.pretrained() \\\n",
    "                .setInputCols(['normalized']) \\\n",
    "                .setOutputCol('lemma')\n",
    "\n",
    "# StopWordsCleaner removes stop words    \n",
    "stopwords_cleaner = StopWordsCleaner() \\\n",
    "                        .setInputCols(['lemma']) \\\n",
    "                        .setOutputCol('clean_lemma') \\\n",
    "                        .setCaseSensitive(False).setStopWords(all_stopwords)\n",
    "\n",
    "# Finisher converts clean tokens to human-readable output\n",
    "finisher = Finisher() \\\n",
    "            .setInputCols(['clean_lemma']) \\\n",
    "            .setCleanAnnotations(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline for fully preprocessing the text\n",
    "pipeline = Pipeline() \\\n",
    "            .setStages([\n",
    "                document_assembler,\n",
    "                tokenizer,\n",
    "                normalizer,\n",
    "                lemmatizer,\n",
    "                stopwords_cleaner,\n",
    "                finisher_tokens,\n",
    "                finisher\n",
    "             ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess questions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+--------------------+--------------------+\n",
      "|question_id|           full_text|        preprocessed|\n",
      "+-----------+--------------------+--------------------+\n",
      "|          0|How does temperat...|[temperature, hum...|\n",
      "|          1|Seasonality of tr...|[seasonality, tra...|\n",
      "|          2|Effectiveness of ...|[effectiveness, i...|\n",
      "|          3|Effectiveness of ...|[effectiveness, p...|\n",
      "|          4|Effectiveness of ...|[effectiveness, s...|\n",
      "|          5|Effectiveness of ...|[effectiveness, c...|\n",
      "|          6|Effectiveness of ...|[effectiveness, m...|\n",
      "|          7|Effectiveness of ...|[effectiveness, c...|\n",
      "|          8|Significant chang...|[change, transmis...|\n",
      "|          9|Effectiveness of ...|[effectiveness, w...|\n",
      "+-----------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# questions = sqlContext.read.format('csv').options(header='true').load('/data/s1847503/SDDM/newdata/questions.csv')\n",
    "questions = sqlContext.read.format('csv').options(header='true').load('/home/rikz/Documents/Master/Semester2/SDDM/data/questions.csv')\n",
    "questions_clean = pipeline.fit(questions).transform(questions)\n",
    "questions_clean = questions_clean.select('question_id', 'full_text', col('finished_clean_lemma').alias('preprocessed'))\n",
    "questions_clean.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Effectiveness of inter inner travel restriction'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Select a question (from 0 to 9)\n",
    "question_num = 2\n",
    "\n",
    "questions_clean = questions_clean.filter(questions_clean.question_id == question_num)\n",
    "q = questions_clean.first().full_text\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_before = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before removing empty papers and duplicates: 510 rows.\n",
      "Removed empty papers\n",
      "Removed duplicates\n",
      "After removing empty papers and duplicates: 509 rows.\n",
      "\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| id|            paper_id|               title|           full_text|text_length|        preprocessed|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "| 54|86c3b8562c3094e95...|Title: A Feminist...|\"Natural disaster...|        498|[natural, disaste...|\n",
      "| 56|55743de644f6eda86...|Anti-microbial im...|Chronic obstructi...|       3601|[chronic, obstruc...|\n",
      "|100|690178e12695c5fdc...|                null|As of this date, ...|       1668|[case, covid, rep...|\n",
      "| 79|5009e8203ed264e9d...|Coronavirus Rever...|\"1 Introduction T...|        316|[introduction, ta...|\n",
      "|278|7f7940bc0ca1e1986...|Update in the tre...|Each year respira...|       7145|[year, respirator...|\n",
      "|396|0d76b14e9ae608525...|Viral Life Cycles...|\"Electron microsc...|        437|[electron, micros...|\n",
      "|234|86e5d3988edd1aa3d...|Potent Antiviral ...|\"The COVID-19 out...|         91|[covid, outbreak,...|\n",
      "|205|3c26c921c42bb4b2c...|Viral Infections ...|\"This article is ...|        300|[article, protect...|\n",
      "| 92|be05821eac40596ed...|Structural charac...|Emerging infectio...|       6360|[emerge, infectio...|\n",
      "|423|bdd32f74cc1be5920...|Anti-In¬£uenzaViru...|The replicative c...|       7366|[replicative, cyc...|\n",
      "|495|263f12984a135507e...|Asymmetric effect...|\"A growing body o...|       1032|[grow, body, argu...|\n",
      "|474|9448084d08566af74...|Tropical Medicine...|\"The global disco...|       4480|[global, discover...|\n",
      "| 13|2ef0ac31fd85f28b5...|SARS-CoV-Encoded ...|In Brief SARS-CoV...|       6708|[sarscov, exacerb...|\n",
      "|294|b4208007d70786df3...|Initial stage of ...|There are various...|        693|[scenario, model,...|\n",
      "|319|1af8c29bad3166064...|                null|explained that in...|       1263|[explain, growth,...|\n",
      "|212|42ce4e5eac6e69f5d...|The development o...|\"Infectious conju...|       1053|[infectious, conj...|\n",
      "|419|fc7c462f4b3c1a7e4...|Human Coronavirus...|\"T he recent iden...|       1306|[identification, ...|\n",
      "|362|5c3bfe9b503d23810...|Potential anti-SA...|A novel coronavir...|       2292|[coronavirus, sev...|\n",
      "| 69|22264c65eb6c14ba9...|Quality control i...|Although the pote...|       1439|[potential, mngs,...|\n",
      "|203|57b1179e96db85fd1...|A novel intracell...|Bovine viral diar...|       5785|[bovine, viral, d...|\n",
      "+---+--------------------+--------------------+--------------------+-----------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Peprocess the data\n",
    "df = pipeline.fit(df).transform(df)\n",
    "df = df.select('*', size('finished_tokens').alias('text_length'))\n",
    "\n",
    "df = df.dropna(subset=['paper_id', 'full_text'])\n",
    "print(\"Removed empty papers\")\n",
    "df = df.dropDuplicates(subset=['paper_id', 'full_text'])\n",
    "print(\"Removed duplicates\")\n",
    "print()\n",
    "\n",
    "df = df.select(\n",
    "                col('_c0').alias('id'),\n",
    "                'paper_id',\n",
    "                'title',\n",
    "                'full_text',\n",
    "                'text_length',\n",
    "                col('finished_clean_lemma').alias('preprocessed')\n",
    "            )\n",
    "\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_after = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Preprocessing time: 14.04532241821289 sec\n"
     ]
    }
   ],
   "source": [
    "print('Preprocessing time: {} sec'.format(time_after-time_before) )\n",
    "# Small dataset: ~15 sec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for papers\n",
    "tf_p = []\n",
    "tf_idf_papers = []\n",
    "\n",
    "tf_p = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(df)\n",
    "\n",
    "tf_idf_papers = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute TF-IDF matrix for questions\n",
    "tf_q = []\n",
    "tf_idf_questions = []\n",
    "\n",
    "tf_q = HashingTF(inputCol='preprocessed', outputCol='tf') \\\n",
    "                    .transform(questions_clean)\n",
    "\n",
    "tf_idf_questions = IDF(inputCol='tf', outputCol='feature') \\\n",
    "                        .fit(tf_p) \\\n",
    "                        .transform(tf_q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Delete any extra columns\n",
    "tf_idf_questions = tf_idf_questions.select('question_id', 'feature')\n",
    "tf_idf_papers = tf_idf_papers.select('id', 'feature')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute L2-norm for papers and questions\n",
    "normalizer_L2 = Normalizer_L2(inputCol='feature', outputCol='norm')\n",
    "tf_idf_papers = normalizer_L2.transform(tf_idf_papers)\n",
    "tf_idf_questions = normalizer_L2.transform(tf_idf_questions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change TF-IDF matrices to block matrices so they can be multiplied\n",
    "from pyspark.mllib.linalg.distributed import IndexedRow, IndexedRowMatrix\n",
    "matrix_q = IndexedRowMatrix(\n",
    "                tf_idf_questions \\\n",
    "                    .select('question_id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.question_id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()\n",
    "\n",
    "matrix_p = IndexedRowMatrix(\n",
    "                tf_idf_papers \\\n",
    "                    .select('id', 'norm') \\\n",
    "                    .rdd.map(lambda row: IndexedRow(row.id, row.norm.toArray()))\n",
    "            ).toBlockMatrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiply the matrices to get the cosine similarity matrix\n",
    "sim_matrix = matrix_p.multiply(matrix_q.transpose())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the similarity matrix to an array\n",
    "sim_matrix = sim_matrix.toLocalMatrix().toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the similarity matrix to a dataframe again to get the most relevant papers for the selected question\n",
    "relevant = sc.parallelize(sim_matrix[:, question_num].tolist()) \\\n",
    "                .zipWithIndex() \\\n",
    "                .toDF(['similarity', 'id'])\n",
    "\n",
    "# Remove questions from the paper list\n",
    "# Sort on cosine similarity\n",
    "# Take the top 10 relevant documents\n",
    "relevant = relevant.select('id', 'similarity') \\\n",
    "                .filter(relevant.id > 9) \\\n",
    "                .sort(col('similarity').desc()) \\\n",
    "                .limit(10)\n",
    "\n",
    "relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Get the data of the 10 most relevant papers in order of relevance\n",
    "relevant_ids = [int(row.id) for row in relevant.collect()]\n",
    "print(\"Query: {}\".format(q))\n",
    "print()\n",
    "print(\"Relevant Papers:\")\n",
    "print()\n",
    "df_relevant = relevant.join(df.filter(df.id.isin(relevant_ids)), on=['id'], how='left_outer') \\\n",
    "                        .select('paper_id', 'similarity')\n",
    "df_relevant.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the summary table with the relevant paper from the metadata\n",
    "df_relevant = df_relevant.join(df_metadata, on=['paper_id'], how='left_outer') \\\n",
    "                            .select('paper_id', 'publish_time', 'title', 'doi', 'journal', 'similarity') \\\n",
    "                            .toPandas() \\\n",
    "                            .sort_values(by='similarity', ascending=False)\n",
    "df_relevant.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Send the summary table to a csv file\n",
    "df_relevant.to_csv('/home/rikz/Documents/Master/Semester2/SDDM/SDDM/summary_tables/{}.csv' \\\n",
    "                   .format(q.lower().replace(' ', '_')), index=False)\n",
    "print(\"Summary table extracted and sent to csv file.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Close Spark Context when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
